
==> Audit <==
|------------|----------------------------------------------------|----------|----------------------------|---------|---------------------|---------------------|
|  Command   |                        Args                        | Profile  |            User            | Version |     Start Time      |      End Time       |
|------------|----------------------------------------------------|----------|----------------------------|---------|---------------------|---------------------|
| start      |                                                    | minikube | DESKTOP-I2QCF1H\Animaznman | v1.35.0 | 10 Apr 25 10:45 PDT | 10 Apr 25 10:47 PDT |
| dashboard  |                                                    | minikube | DESKTOP-I2QCF1H\Animaznman | v1.35.0 | 10 Apr 25 10:47 PDT |                     |
| start      |                                                    | minikube | DESKTOP-I2QCF1H\Animaznman | v1.35.0 | 13 Apr 25 16:11 PDT | 13 Apr 25 16:12 PDT |
| dashboard  |                                                    | minikube | DESKTOP-I2QCF1H\Animaznman | v1.35.0 | 13 Apr 25 16:12 PDT |                     |
| docker-env |                                                    | minikube | DESKTOP-I2QCF1H\Animaznman | v1.35.0 | 13 Apr 25 16:16 PDT | 13 Apr 25 16:16 PDT |
| docker-env | set DOCKER_TLS_VERIFY=1set                         | minikube | DESKTOP-I2QCF1H\Animaznman | v1.35.0 | 13 Apr 25 16:18 PDT | 13 Apr 25 16:18 PDT |
|            | DOCKER_HOST=tcp://2376 set                         |          |                            |         |                     |                     |
|            | DOCKER_CERT_PATH=C:\Users\anima\.minikube\certsset |          |                            |         |                     |                     |
|            | DOCKER_API_VERSION=1.40                            |          |                            |         |                     |                     |
| dashboard  |                                                    | minikube | DESKTOP-I2QCF1H\Animaznman | v1.35.0 | 13 Apr 25 16:57 PDT |                     |
| service    | mlflow-service                                     | minikube | DESKTOP-I2QCF1H\Animaznman | v1.35.0 | 13 Apr 25 16:59 PDT |                     |
| docker-env | --shell cmd                                        | minikube | DESKTOP-I2QCF1H\Animaznman | v1.35.0 | 13 Apr 25 17:40 PDT | 13 Apr 25 17:40 PDT |
| docker-env | --shell cmd                                        | minikube | DESKTOP-I2QCF1H\Animaznman | v1.35.0 | 13 Apr 25 17:44 PDT |                     |
| delete     |                                                    | minikube | DESKTOP-I2QCF1H\Animaznman | v1.35.0 | 13 Apr 25 17:49 PDT | 13 Apr 25 17:49 PDT |
| start      |                                                    | minikube | DESKTOP-I2QCF1H\Animaznman | v1.35.0 | 13 Apr 25 17:49 PDT |                     |
| start      | --driver=docker --cpus=4                           | minikube | DESKTOP-I2QCF1H\Animaznman | v1.35.0 | 13 Apr 25 18:03 PDT |                     |
|            | --memory=8192                                      |          |                            |         |                     |                     |
| delete     |                                                    | minikube | DESKTOP-I2QCF1H\Animaznman | v1.35.0 | 13 Apr 25 18:04 PDT | 13 Apr 25 18:04 PDT |
| start      | --driver=docker --cpus=4                           | minikube | DESKTOP-I2QCF1H\Animaznman | v1.35.0 | 13 Apr 25 18:04 PDT |                     |
|            | --memory=8192                                      |          |                            |         |                     |                     |
| start      | --driver=docker --cpus=4                           | minikube | DESKTOP-I2QCF1H\Animaznman | v1.35.0 | 13 Apr 25 18:04 PDT |                     |
|            | --memory=7800                                      |          |                            |         |                     |                     |
| start      | --driver=docker                                    | minikube | DESKTOP-I2QCF1H\Animaznman | v1.35.0 | 13 Apr 25 18:13 PDT |                     |
| start      | --driver=docker --cpus=4                           | minikube | DESKTOP-I2QCF1H\Animaznman | v1.35.0 | 13 Apr 25 18:13 PDT |                     |
|            | --memory=7800                                      |          |                            |         |                     |                     |
| start      | --driver=docker --cpus=4                           | minikube | DESKTOP-I2QCF1H\Animaznman | v1.35.0 | 13 Apr 25 18:14 PDT |                     |
|            | --memory=7800                                      |          |                            |         |                     |                     |
| stop       |                                                    | minikube | DESKTOP-I2QCF1H\Animaznman | v1.35.0 | 13 Apr 25 18:17 PDT | 13 Apr 25 18:17 PDT |
| start      | --driver=docker --cpus=4                           | minikube | DESKTOP-I2QCF1H\Animaznman | v1.35.0 | 13 Apr 25 18:17 PDT |                     |
|            | --memory=7800 --docker-env                         |          |                            |         |                     |                     |
|            | NO_PROXY=*                                         |          |                            |         |                     |                     |
| stop       |                                                    | minikube | DESKTOP-I2QCF1H\Animaznman | v1.35.0 | 13 Apr 25 18:19 PDT | 13 Apr 25 18:19 PDT |
| start      | --driver=docker --cpus=4                           | minikube | DESKTOP-I2QCF1H\Animaznman | v1.35.0 | 13 Apr 25 18:20 PDT |                     |
|            | --memory=7800                                      |          |                            |         |                     |                     |
| start      |                                                    | minikube | DESKTOP-I2QCF1H\Animaznman | v1.35.0 | 13 Apr 25 19:06 PDT |                     |
| dashboard  |                                                    | minikube | DESKTOP-I2QCF1H\Animaznman | v1.35.0 | 13 Apr 25 19:12 PDT |                     |
| start      |                                                    | minikube | DESKTOP-I2QCF1H\Animaznman | v1.35.0 | 13 Apr 25 19:29 PDT |                     |
| start      | NO_PROXY=*                                         | minikube | DESKTOP-I2QCF1H\Animaznman | v1.35.0 | 13 Apr 25 19:30 PDT |                     |
| dashboard  |                                                    | minikube | DESKTOP-I2QCF1H\Animaznman | v1.35.0 | 13 Apr 25 19:34 PDT |                     |
| start      | --driver=docker --cpus=4                           | minikube | DESKTOP-I2QCF1H\Animaznman | v1.35.0 | 13 Apr 25 19:35 PDT |                     |
|            | --memory=7800                                      |          |                            |         |                     |                     |
| start      | --driver=docker --cpus=4                           | minikube | DESKTOP-I2QCF1H\Animaznman | v1.35.0 | 13 Apr 25 20:13 PDT |                     |
|            | --memory=7800                                      |          |                            |         |                     |                     |
| dashboard  |                                                    | minikube | DESKTOP-I2QCF1H\Animaznman | v1.35.0 | 13 Apr 25 20:21 PDT |                     |
| dashboard  |                                                    | minikube | DESKTOP-I2QCF1H\Animaznman | v1.35.0 | 13 Apr 25 21:22 PDT |                     |
| start      | --driver=docker --cpus=4                           | minikube | DESKTOP-I2QCF1H\Animaznman | v1.35.0 | 13 Apr 25 21:25 PDT |                     |
|            | --memory=7800                                      |          |                            |         |                     |                     |
| addons     | enable default-storageclass                        | minikube | DESKTOP-I2QCF1H\Animaznman | v1.35.0 | 13 Apr 25 21:31 PDT |                     |
| addons     | list                                               | minikube | DESKTOP-I2QCF1H\Animaznman | v1.35.0 | 13 Apr 25 21:31 PDT | 13 Apr 25 21:31 PDT |
| addons     | enable default-storageclass                        | minikube | DESKTOP-I2QCF1H\Animaznman | v1.35.0 | 13 Apr 25 21:32 PDT |                     |
| stop       |                                                    | minikube | DESKTOP-I2QCF1H\Animaznman | v1.35.0 | 13 Apr 25 21:39 PDT | 13 Apr 25 21:39 PDT |
| start      | --driver=docker --cpus=4                           | minikube | DESKTOP-I2QCF1H\Animaznman | v1.35.0 | 13 Apr 25 21:39 PDT |                     |
|            | --memory=7800                                      |          |                            |         |                     |                     |
| delete     |                                                    | minikube | DESKTOP-I2QCF1H\Animaznman | v1.35.0 | 13 Apr 25 21:58 PDT | 13 Apr 25 21:58 PDT |
| start      | --driver=docker --cpus=4                           | minikube | DESKTOP-I2QCF1H\Animaznman | v1.35.0 | 13 Apr 25 21:58 PDT | 13 Apr 25 21:58 PDT |
|            | --memory=7800                                      |          |                            |         |                     |                     |
|------------|----------------------------------------------------|----------|----------------------------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2025/04/13 21:58:28
Running on machine: DESKTOP-I2QCF1H
Binary: Built with gc go1.23.4 for windows/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0413 21:58:28.141320    8940 out.go:345] Setting OutFile to fd 88 ...
I0413 21:58:28.142321    8940 out.go:392] TERM=,COLORTERM=, which probably does not support color
I0413 21:58:28.142321    8940 out.go:358] Setting ErrFile to fd 92...
I0413 21:58:28.142321    8940 out.go:392] TERM=,COLORTERM=, which probably does not support color
I0413 21:58:28.158248    8940 out.go:352] Setting JSON to false
I0413 21:58:28.162560    8940 start.go:129] hostinfo: {"hostname":"DESKTOP-I2QCF1H","uptime":13692,"bootTime":1744593015,"procs":324,"os":"windows","platform":"Microsoft Windows 10 Home","platformFamily":"Standalone Workstation","platformVersion":"10.0.19045.5737 Build 19045.5737","kernelVersion":"10.0.19045.5737 Build 19045.5737","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"0e50ae36-59d9-4ffe-a201-8e166a345ee4"}
W0413 21:58:28.162560    8940 start.go:137] gopshost.Virtualization returned error: not implemented yet
I0413 21:58:28.163597    8940 out.go:177] * minikube v1.35.0 on Microsoft Windows 10 Home 10.0.19045.5737 Build 19045.5737
I0413 21:58:28.164634    8940 notify.go:220] Checking for updates...
I0413 21:58:28.165155    8940 driver.go:394] Setting default libvirt URI to qemu:///system
I0413 21:58:28.236499    8940 docker.go:123] docker version: linux-28.0.4:Docker Desktop 4.40.0 (187762)
I0413 21:58:28.251757    8940 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0413 21:58:28.383395    8940 info.go:266] docker info: {ID:566ff656-eebf-4fd0-9d57-a22bcb7d5f4c Containers:3 ContainersRunning:0 ContainersPaused:0 ContainersStopped:3 Images:4 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:71 OomKillDisable:true NGoroutines:91 SystemTime:2025-04-14 04:58:28.080738571 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:18 KernelVersion:5.15.167.4-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:12 MemTotal:8219967488 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:28.0.4 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:753481ec61c7c8955a23d6ff7bc8e4daed455734 Expected:753481ec61c7c8955a23d6ff7bc8e4daed455734} RuncCommit:{ID:v1.2.5-0-g59923ef Expected:v1.2.5-0-g59923ef} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: DOCKER_INSECURE_NO_IPTABLES_RAW is set WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:C:\Program Files\Docker\cli-plugins\docker-ai.exe SchemaVersion:0.1.0 ShortDescription:Docker AI Agent - Ask Gordon Vendor:Docker Inc. Version:v1.1.3] map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.22.0-desktop.1] map[Name:cloud Path:C:\Program Files\Docker\cli-plugins\docker-cloud.exe SchemaVersion:0.1.0 ShortDescription:Docker Cloud Vendor:Docker Inc. Version:0.2.20] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.34.0-desktop.1] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.38] map[Name:desktop Path:C:\Program Files\Docker\cli-plugins\docker-desktop.exe SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands (Beta) Vendor:Docker Inc. Version:v0.1.6] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.27] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.17.0]] Warnings:<nil>}}
I0413 21:58:28.384443    8940 out.go:177] * Using the docker driver based on user configuration
I0413 21:58:28.385491    8940 start.go:297] selected driver: docker
I0413 21:58:28.385491    8940 start.go:901] validating driver "docker" against <nil>
I0413 21:58:28.385491    8940 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0413 21:58:28.464601    8940 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0413 21:58:28.585637    8940 info.go:266] docker info: {ID:566ff656-eebf-4fd0-9d57-a22bcb7d5f4c Containers:3 ContainersRunning:0 ContainersPaused:0 ContainersStopped:3 Images:4 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:71 OomKillDisable:true NGoroutines:91 SystemTime:2025-04-14 04:58:28.29854073 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:18 KernelVersion:5.15.167.4-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:12 MemTotal:8219967488 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:28.0.4 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:753481ec61c7c8955a23d6ff7bc8e4daed455734 Expected:753481ec61c7c8955a23d6ff7bc8e4daed455734} RuncCommit:{ID:v1.2.5-0-g59923ef Expected:v1.2.5-0-g59923ef} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: DOCKER_INSECURE_NO_IPTABLES_RAW is set WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:C:\Program Files\Docker\cli-plugins\docker-ai.exe SchemaVersion:0.1.0 ShortDescription:Docker AI Agent - Ask Gordon Vendor:Docker Inc. Version:v1.1.3] map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.22.0-desktop.1] map[Name:cloud Path:C:\Program Files\Docker\cli-plugins\docker-cloud.exe SchemaVersion:0.1.0 ShortDescription:Docker Cloud Vendor:Docker Inc. Version:0.2.20] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.34.0-desktop.1] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.38] map[Name:desktop Path:C:\Program Files\Docker\cli-plugins\docker-desktop.exe SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands (Beta) Vendor:Docker Inc. Version:v0.1.6] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.27] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.17.0]] Warnings:<nil>}}
I0413 21:58:28.586162    8940 start_flags.go:310] no existing cluster config was found, will generate one from the flags 
I0413 21:58:28.587206    8940 start_flags.go:929] Wait components to verify : map[apiserver:true system_pods:true]
I0413 21:58:28.587735    8940 out.go:177] * Using Docker Desktop driver with root privileges
I0413 21:58:28.588791    8940 cni.go:84] Creating CNI manager for ""
I0413 21:58:28.588791    8940 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0413 21:58:28.588791    8940 start_flags.go:319] Found "bridge CNI" CNI - setting NetworkPlugin=cni
I0413 21:58:28.588791    8940 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:7800 CPUs:4 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\anima:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0413 21:58:28.589314    8940 out.go:177] * Starting "minikube" primary control-plane node in "minikube" cluster
I0413 21:58:28.590357    8940 cache.go:121] Beginning downloading kic base image for docker with docker
I0413 21:58:28.590881    8940 out.go:177] * Pulling base image v0.0.46 ...
I0413 21:58:28.592016    8940 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0413 21:58:28.592016    8940 image.go:81] Checking for gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local docker daemon
I0413 21:58:28.592520    8940 preload.go:146] Found local preload: C:\Users\anima\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4
I0413 21:58:28.592520    8940 cache.go:56] Caching tarball of preloaded images
I0413 21:58:28.592537    8940 preload.go:172] Found C:\Users\anima\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0413 21:58:28.592537    8940 cache.go:59] Finished verifying existence of preloaded tar for v1.32.0 on docker
I0413 21:58:28.593061    8940 profile.go:143] Saving config to C:\Users\anima\.minikube\profiles\minikube\config.json ...
I0413 21:58:28.593061    8940 lock.go:35] WriteFile acquiring C:\Users\anima\.minikube\profiles\minikube\config.json: {Name:mk09234d297169cd6623b60d23f15ab92ac373f8 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0413 21:58:28.646110    8940 image.go:100] Found gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local docker daemon, skipping pull
I0413 21:58:28.646617    8940 cache.go:145] gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 exists in daemon, skipping load
I0413 21:58:28.646635    8940 cache.go:227] Successfully downloaded all kic artifacts
I0413 21:58:28.646635    8940 start.go:360] acquireMachinesLock for minikube: {Name:mkf7da0f6ce33ce6301816eb6aae86ca88a61be4 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0413 21:58:28.646635    8940 start.go:364] duration metric: took 0s to acquireMachinesLock for "minikube"
I0413 21:58:28.646635    8940 start.go:93] Provisioning new machine with config: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:7800 CPUs:4 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\anima:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} &{Name: IP: Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0413 21:58:28.646635    8940 start.go:125] createHost starting for "" (driver="docker")
I0413 21:58:28.647685    8940 out.go:235] * Creating docker container (CPUs=4, Memory=7800MB) ...
I0413 21:58:28.648206    8940 start.go:159] libmachine.API.Create for "minikube" (driver="docker")
I0413 21:58:28.648206    8940 client.go:168] LocalClient.Create starting
I0413 21:58:28.648206    8940 main.go:141] libmachine: Reading certificate data from C:\Users\anima\.minikube\certs\ca.pem
I0413 21:58:28.648206    8940 main.go:141] libmachine: Decoding PEM data...
I0413 21:58:28.648206    8940 main.go:141] libmachine: Parsing certificate...
I0413 21:58:28.648739    8940 main.go:141] libmachine: Reading certificate data from C:\Users\anima\.minikube\certs\cert.pem
I0413 21:58:28.648739    8940 main.go:141] libmachine: Decoding PEM data...
I0413 21:58:28.648739    8940 main.go:141] libmachine: Parsing certificate...
I0413 21:58:28.664659    8940 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
W0413 21:58:28.698170    8940 cli_runner.go:211] docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}" returned with exit code 1
I0413 21:58:28.713572    8940 network_create.go:284] running [docker network inspect minikube] to gather additional debugging logs...
I0413 21:58:28.713572    8940 cli_runner.go:164] Run: docker network inspect minikube
W0413 21:58:28.748155    8940 cli_runner.go:211] docker network inspect minikube returned with exit code 1
I0413 21:58:28.748155    8940 network_create.go:287] error running [docker network inspect minikube]: docker network inspect minikube: exit status 1
stdout:
[]

stderr:
Error response from daemon: network minikube not found
I0413 21:58:28.748155    8940 network_create.go:289] output of [docker network inspect minikube]: -- stdout --
[]

-- /stdout --
** stderr ** 
Error response from daemon: network minikube not found

** /stderr **
I0413 21:58:28.763957    8940 cli_runner.go:164] Run: docker network inspect bridge --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0413 21:58:28.819425    8940 network.go:206] using free private subnet 192.168.49.0/24: &{IP:192.168.49.0 Netmask:255.255.255.0 Prefix:24 CIDR:192.168.49.0/24 Gateway:192.168.49.1 ClientMin:192.168.49.2 ClientMax:192.168.49.254 Broadcast:192.168.49.255 IsPrivate:true Interface:{IfaceName: IfaceIPv4: IfaceMTU:0 IfaceMAC:} reservation:0xc001819cb0}
I0413 21:58:28.819425    8940 network_create.go:124] attempt to create docker network minikube 192.168.49.0/24 with gateway 192.168.49.1 and MTU of 1500 ...
I0413 21:58:28.834097    8940 cli_runner.go:164] Run: docker network create --driver=bridge --subnet=192.168.49.0/24 --gateway=192.168.49.1 -o --ip-masq -o --icc -o com.docker.network.driver.mtu=1500 --label=created_by.minikube.sigs.k8s.io=true --label=name.minikube.sigs.k8s.io=minikube minikube
I0413 21:58:28.892377    8940 network_create.go:108] docker network minikube 192.168.49.0/24 created
I0413 21:58:28.892377    8940 kic.go:121] calculated static IP "192.168.49.2" for the "minikube" container
I0413 21:58:28.923043    8940 cli_runner.go:164] Run: docker ps -a --format {{.Names}}
I0413 21:58:28.979149    8940 cli_runner.go:164] Run: docker volume create minikube --label name.minikube.sigs.k8s.io=minikube --label created_by.minikube.sigs.k8s.io=true
I0413 21:58:29.014456    8940 oci.go:103] Successfully created a docker volume minikube
I0413 21:58:29.030232    8940 cli_runner.go:164] Run: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 -d /var/lib
I0413 21:58:29.956048    8940 oci.go:107] Successfully prepared a docker volume minikube
I0413 21:58:29.956062    8940 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0413 21:58:29.956073    8940 kic.go:194] Starting extracting preloaded images to volume ...
I0413 21:58:29.972759    8940 cli_runner.go:164] Run: docker run --rm --entrypoint /usr/bin/tar -v C:\Users\anima\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 -I lz4 -xf /preloaded.tar -C /extractDir
I0413 21:58:36.189889    8940 cli_runner.go:217] Completed: docker run --rm --entrypoint /usr/bin/tar -v C:\Users\anima\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 -I lz4 -xf /preloaded.tar -C /extractDir: (6.2171296s)
I0413 21:58:36.189889    8940 kic.go:203] duration metric: took 6.2338152s to extract preloaded images to volume ...
I0413 21:58:36.204659    8940 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0413 21:58:36.352809    8940 info.go:266] docker info: {ID:566ff656-eebf-4fd0-9d57-a22bcb7d5f4c Containers:3 ContainersRunning:0 ContainersPaused:0 ContainersStopped:3 Images:4 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:72 OomKillDisable:true NGoroutines:91 SystemTime:2025-04-14 04:58:36.56274602 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:18 KernelVersion:5.15.167.4-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:12 MemTotal:8219967488 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:28.0.4 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:753481ec61c7c8955a23d6ff7bc8e4daed455734 Expected:753481ec61c7c8955a23d6ff7bc8e4daed455734} RuncCommit:{ID:v1.2.5-0-g59923ef Expected:v1.2.5-0-g59923ef} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: DOCKER_INSECURE_NO_IPTABLES_RAW is set WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:C:\Program Files\Docker\cli-plugins\docker-ai.exe SchemaVersion:0.1.0 ShortDescription:Docker AI Agent - Ask Gordon Vendor:Docker Inc. Version:v1.1.3] map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.22.0-desktop.1] map[Name:cloud Path:C:\Program Files\Docker\cli-plugins\docker-cloud.exe SchemaVersion:0.1.0 ShortDescription:Docker Cloud Vendor:Docker Inc. Version:0.2.20] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.34.0-desktop.1] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.38] map[Name:desktop Path:C:\Program Files\Docker\cli-plugins\docker-desktop.exe SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands (Beta) Vendor:Docker Inc. Version:v0.1.6] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.27] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.17.0]] Warnings:<nil>}}
I0413 21:58:36.373136    8940 cli_runner.go:164] Run: docker info --format "'{{json .SecurityOptions}}'"
I0413 21:58:36.518502    8940 cli_runner.go:164] Run: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=7800mb --memory-swap=7800mb --cpus=4 -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279
I0413 21:58:36.804467    8940 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Running}}
I0413 21:58:36.862973    8940 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0413 21:58:36.914815    8940 cli_runner.go:164] Run: docker exec minikube stat /var/lib/dpkg/alternatives/iptables
I0413 21:58:36.986258    8940 oci.go:144] the created container "minikube" has a running status.
I0413 21:58:36.986776    8940 kic.go:225] Creating ssh key for kic: C:\Users\anima\.minikube\machines\minikube\id_rsa...
I0413 21:58:37.048751    8940 kic_runner.go:191] docker (temp): C:\Users\anima\.minikube\machines\minikube\id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I0413 21:58:37.256159    8940 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0413 21:58:37.323388    8940 kic_runner.go:93] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I0413 21:58:37.323388    8940 kic_runner.go:114] Args: [docker exec --privileged minikube chown docker:docker /home/docker/.ssh/authorized_keys]
I0413 21:58:37.397354    8940 kic.go:265] ensuring only current user has permissions to key file located at : C:\Users\anima\.minikube\machines\minikube\id_rsa...
I0413 21:58:38.326351    8940 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0413 21:58:38.358764    8940 machine.go:93] provisionDockerMachine start ...
I0413 21:58:38.373405    8940 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0413 21:58:38.415690    8940 main.go:141] libmachine: Using SSH client type: native
I0413 21:58:38.422995    8940 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1055360] 0x1057ea0 <nil>  [] 0s} 127.0.0.1 53618 <nil> <nil>}
I0413 21:58:38.422995    8940 main.go:141] libmachine: About to run SSH command:
hostname
I0413 21:58:38.555528    8940 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0413 21:58:38.555528    8940 ubuntu.go:169] provisioning hostname "minikube"
I0413 21:58:38.570603    8940 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0413 21:58:38.612572    8940 main.go:141] libmachine: Using SSH client type: native
I0413 21:58:38.612572    8940 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1055360] 0x1057ea0 <nil>  [] 0s} 127.0.0.1 53618 <nil> <nil>}
I0413 21:58:38.612572    8940 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0413 21:58:38.742441    8940 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0413 21:58:38.757011    8940 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0413 21:58:38.799085    8940 main.go:141] libmachine: Using SSH client type: native
I0413 21:58:38.799085    8940 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1055360] 0x1057ea0 <nil>  [] 0s} 127.0.0.1 53618 <nil> <nil>}
I0413 21:58:38.799589    8940 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0413 21:58:38.923032    8940 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0413 21:58:38.923032    8940 ubuntu.go:175] set auth options {CertDir:C:\Users\anima\.minikube CaCertPath:C:\Users\anima\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\anima\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\anima\.minikube\machines\server.pem ServerKeyPath:C:\Users\anima\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\anima\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\anima\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\anima\.minikube}
I0413 21:58:38.923032    8940 ubuntu.go:177] setting up certificates
I0413 21:58:38.923032    8940 provision.go:84] configureAuth start
I0413 21:58:38.937642    8940 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0413 21:58:38.971772    8940 provision.go:143] copyHostCerts
I0413 21:58:38.972289    8940 exec_runner.go:144] found C:\Users\anima\.minikube/cert.pem, removing ...
I0413 21:58:38.972289    8940 exec_runner.go:203] rm: C:\Users\anima\.minikube\cert.pem
I0413 21:58:38.972289    8940 exec_runner.go:151] cp: C:\Users\anima\.minikube\certs\cert.pem --> C:\Users\anima\.minikube/cert.pem (1131 bytes)
I0413 21:58:38.972808    8940 exec_runner.go:144] found C:\Users\anima\.minikube/key.pem, removing ...
I0413 21:58:38.972808    8940 exec_runner.go:203] rm: C:\Users\anima\.minikube\key.pem
I0413 21:58:38.972808    8940 exec_runner.go:151] cp: C:\Users\anima\.minikube\certs\key.pem --> C:\Users\anima\.minikube/key.pem (1675 bytes)
I0413 21:58:38.973325    8940 exec_runner.go:144] found C:\Users\anima\.minikube/ca.pem, removing ...
I0413 21:58:38.973325    8940 exec_runner.go:203] rm: C:\Users\anima\.minikube\ca.pem
I0413 21:58:38.973845    8940 exec_runner.go:151] cp: C:\Users\anima\.minikube\certs\ca.pem --> C:\Users\anima\.minikube/ca.pem (1086 bytes)
I0413 21:58:38.973845    8940 provision.go:117] generating server cert: C:\Users\anima\.minikube\machines\server.pem ca-key=C:\Users\anima\.minikube\certs\ca.pem private-key=C:\Users\anima\.minikube\certs\ca-key.pem org=Animaznman.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0413 21:58:39.068605    8940 provision.go:177] copyRemoteCerts
I0413 21:58:39.088290    8940 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0413 21:58:39.102880    8940 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0413 21:58:39.139169    8940 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:53618 SSHKeyPath:C:\Users\anima\.minikube\machines\minikube\id_rsa Username:docker}
I0413 21:58:39.235720    8940 ssh_runner.go:362] scp C:\Users\anima\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1086 bytes)
I0413 21:58:39.253905    8940 ssh_runner.go:362] scp C:\Users\anima\.minikube\machines\server.pem --> /etc/docker/server.pem (1188 bytes)
I0413 21:58:39.269837    8940 ssh_runner.go:362] scp C:\Users\anima\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0413 21:58:39.285840    8940 provision.go:87] duration metric: took 362.8089ms to configureAuth
I0413 21:58:39.285840    8940 ubuntu.go:193] setting minikube options for container-runtime
I0413 21:58:39.286362    8940 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0413 21:58:39.300453    8940 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0413 21:58:39.344269    8940 main.go:141] libmachine: Using SSH client type: native
I0413 21:58:39.344269    8940 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1055360] 0x1057ea0 <nil>  [] 0s} 127.0.0.1 53618 <nil> <nil>}
I0413 21:58:39.344269    8940 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0413 21:58:39.467895    8940 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0413 21:58:39.467895    8940 ubuntu.go:71] root file system type: overlay
I0413 21:58:39.467895    8940 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0413 21:58:39.482547    8940 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0413 21:58:39.527813    8940 main.go:141] libmachine: Using SSH client type: native
I0413 21:58:39.527813    8940 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1055360] 0x1057ea0 <nil>  [] 0s} 127.0.0.1 53618 <nil> <nil>}
I0413 21:58:39.527813    8940 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0413 21:58:39.662717    8940 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0413 21:58:39.677259    8940 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0413 21:58:39.719563    8940 main.go:141] libmachine: Using SSH client type: native
I0413 21:58:39.719563    8940 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1055360] 0x1057ea0 <nil>  [] 0s} 127.0.0.1 53618 <nil> <nil>}
I0413 21:58:39.719563    8940 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0413 21:58:42.005633    8940 main.go:141] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2024-12-17 15:44:19.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2025-04-14 04:58:40.085489617 +0000
@@ -1,46 +1,49 @@
 [Unit]
 Description=Docker Application Container Engine
 Documentation=https://docs.docker.com
-After=network-online.target docker.socket firewalld.service containerd.service time-set.target
-Wants=network-online.target containerd.service
+BindsTo=containerd.service
+After=network-online.target firewalld.service containerd.service
+Wants=network-online.target
 Requires=docker.socket
+StartLimitBurst=3
+StartLimitIntervalSec=60
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutStartSec=0
-RestartSec=2
-Restart=always
+Restart=on-failure
 
-# Note that StartLimit* options were moved from "Service" to "Unit" in systemd 229.
-# Both the old, and new location are accepted by systemd 229 and up, so using the old location
-# to make them work for either version of systemd.
-StartLimitBurst=3
 
-# Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230.
-# Both the old, and new name are accepted by systemd 230 and up, so using the old name to make
-# this option work for either version of systemd.
-StartLimitInterval=60s
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
+ExecReload=/bin/kill -s HUP $MAINPID
 
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
+LimitNOFILE=infinity
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
 
 # kill only the docker process, not all processes in the cgroup
 KillMode=process
-OOMScoreAdjust=-500
 
 [Install]
 WantedBy=multi-user.target
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I0413 21:58:42.005633    8940 machine.go:96] duration metric: took 3.6468689s to provisionDockerMachine
I0413 21:58:42.005633    8940 client.go:171] duration metric: took 13.3574271s to LocalClient.Create
I0413 21:58:42.005633    8940 start.go:167] duration metric: took 13.3574271s to libmachine.API.Create "minikube"
I0413 21:58:42.005633    8940 start.go:293] postStartSetup for "minikube" (driver="docker")
I0413 21:58:42.005633    8940 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0413 21:58:42.025475    8940 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0413 21:58:42.039886    8940 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0413 21:58:42.074562    8940 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:53618 SSHKeyPath:C:\Users\anima\.minikube\machines\minikube\id_rsa Username:docker}
I0413 21:58:42.171962    8940 ssh_runner.go:195] Run: cat /etc/os-release
I0413 21:58:42.174812    8940 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0413 21:58:42.174812    8940 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0413 21:58:42.174812    8940 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0413 21:58:42.174812    8940 info.go:137] Remote host: Ubuntu 22.04.5 LTS
I0413 21:58:42.174812    8940 filesync.go:126] Scanning C:\Users\anima\.minikube\addons for local assets ...
I0413 21:58:42.175337    8940 filesync.go:126] Scanning C:\Users\anima\.minikube\files for local assets ...
I0413 21:58:42.175337    8940 start.go:296] duration metric: took 169.7034ms for postStartSetup
I0413 21:58:42.191608    8940 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0413 21:58:42.223258    8940 profile.go:143] Saving config to C:\Users\anima\.minikube\profiles\minikube\config.json ...
I0413 21:58:42.240735    8940 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0413 21:58:42.255170    8940 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0413 21:58:42.287915    8940 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:53618 SSHKeyPath:C:\Users\anima\.minikube\machines\minikube\id_rsa Username:docker}
I0413 21:58:42.386288    8940 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0413 21:58:42.390012    8940 start.go:128] duration metric: took 13.7433769s to createHost
I0413 21:58:42.390012    8940 start.go:83] releasing machines lock for "minikube", held for 13.7433769s
I0413 21:58:42.405137    8940 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0413 21:58:42.439740    8940 ssh_runner.go:195] Run: curl.exe -sS -m 2 https://registry.k8s.io/
I0413 21:58:42.451227    8940 ssh_runner.go:195] Run: cat /version.json
I0413 21:58:42.455985    8940 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0413 21:58:42.466506    8940 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0413 21:58:42.488599    8940 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:53618 SSHKeyPath:C:\Users\anima\.minikube\machines\minikube\id_rsa Username:docker}
I0413 21:58:42.498228    8940 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:53618 SSHKeyPath:C:\Users\anima\.minikube\machines\minikube\id_rsa Username:docker}
W0413 21:58:42.570785    8940 start.go:867] [curl.exe -sS -m 2 https://registry.k8s.io/] failed: curl.exe -sS -m 2 https://registry.k8s.io/: Process exited with status 127
stdout:

stderr:
bash: line 1: curl.exe: command not found
I0413 21:58:42.612498    8940 ssh_runner.go:195] Run: systemctl --version
I0413 21:58:42.629078    8940 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0413 21:58:42.653545    8940 ssh_runner.go:195] Run: sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
W0413 21:58:42.661529    8940 start.go:439] unable to name loopback interface in configureRuntimes: unable to patch loopback cni config "/etc/cni/net.d/*loopback.conf*": sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;: Process exited with status 1
stdout:

stderr:
find: '\\etc\\cni\\net.d': No such file or directory
W0413 21:58:42.673493    8940 out.go:270] ! Failing to connect to https://registry.k8s.io/ from inside the minikube container
W0413 21:58:42.674012    8940 out.go:270] * To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/
I0413 21:58:42.680744    8940 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0413 21:58:42.701752    8940 cni.go:262] disabled [/etc/cni/net.d/100-crio-bridge.conf, /etc/cni/net.d/87-podman-bridge.conflist] bridge cni config(s)
I0413 21:58:42.701752    8940 start.go:495] detecting cgroup driver to use...
I0413 21:58:42.701752    8940 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0413 21:58:42.701752    8940 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0413 21:58:42.724710    8940 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I0413 21:58:42.744803    8940 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0413 21:58:42.751655    8940 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I0413 21:58:42.764147    8940 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0413 21:58:42.783744    8940 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0413 21:58:42.803365    8940 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0413 21:58:42.822949    8940 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0413 21:58:42.842272    8940 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0413 21:58:42.861511    8940 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0413 21:58:42.881293    8940 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0413 21:58:42.900226    8940 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0413 21:58:42.927008    8940 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0413 21:58:42.952907    8940 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0413 21:58:42.978034    8940 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0413 21:58:43.080987    8940 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0413 21:58:43.172591    8940 start.go:495] detecting cgroup driver to use...
I0413 21:58:43.172591    8940 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0413 21:58:43.194902    8940 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0413 21:58:43.203641    8940 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0413 21:58:43.224300    8940 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0413 21:58:43.233060    8940 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0413 21:58:43.256048    8940 ssh_runner.go:195] Run: which cri-dockerd
I0413 21:58:43.278049    8940 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0413 21:58:43.285204    8940 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)
I0413 21:58:43.320284    8940 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0413 21:58:43.423760    8940 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0413 21:58:43.508227    8940 docker.go:574] configuring docker to use "cgroupfs" as cgroup driver...
I0413 21:58:43.508227    8940 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0413 21:58:43.540634    8940 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0413 21:58:43.639901    8940 ssh_runner.go:195] Run: sudo systemctl restart docker
I0413 21:58:46.704704    8940 ssh_runner.go:235] Completed: sudo systemctl restart docker: (3.0648033s)
I0413 21:58:46.724030    8940 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0413 21:58:46.753212    8940 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0413 21:58:46.781264    8940 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0413 21:58:46.879398    8940 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0413 21:58:46.990137    8940 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0413 21:58:47.093996    8940 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0413 21:58:47.122682    8940 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0413 21:58:47.150266    8940 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0413 21:58:47.236178    8940 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0413 21:58:47.293541    8940 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0413 21:58:47.306075    8940 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0413 21:58:47.309025    8940 start.go:563] Will wait 60s for crictl version
I0413 21:58:47.322040    8940 ssh_runner.go:195] Run: which crictl
I0413 21:58:47.344136    8940 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0413 21:58:47.468747    8940 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  27.4.1
RuntimeApiVersion:  v1
I0413 21:58:47.483411    8940 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0413 21:58:47.599977    8940 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0413 21:58:47.617216    8940 out.go:235] * Preparing Kubernetes v1.32.0 on Docker 27.4.1 ...
I0413 21:58:47.632902    8940 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I0413 21:58:47.739895    8940 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I0413 21:58:47.752400    8940 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I0413 21:58:47.755355    8940 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0413 21:58:47.777389    8940 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0413 21:58:47.810000    8940 kubeadm.go:883] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:7800 CPUs:4 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\anima:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0413 21:58:47.810000    8940 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0413 21:58:47.824757    8940 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0413 21:58:47.839122    8940 docker.go:689] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.32.0
registry.k8s.io/kube-controller-manager:v1.32.0
registry.k8s.io/kube-scheduler:v1.32.0
registry.k8s.io/kube-proxy:v1.32.0
registry.k8s.io/etcd:3.5.16-0
registry.k8s.io/coredns/coredns:v1.11.3
registry.k8s.io/pause:3.10
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0413 21:58:47.839122    8940 docker.go:619] Images already preloaded, skipping extraction
I0413 21:58:47.853833    8940 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0413 21:58:47.869980    8940 docker.go:689] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.32.0
registry.k8s.io/kube-controller-manager:v1.32.0
registry.k8s.io/kube-scheduler:v1.32.0
registry.k8s.io/kube-proxy:v1.32.0
registry.k8s.io/etcd:3.5.16-0
registry.k8s.io/coredns/coredns:v1.11.3
registry.k8s.io/pause:3.10
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0413 21:58:47.869980    8940 cache_images.go:84] Images are preloaded, skipping loading
I0413 21:58:47.869980    8940 kubeadm.go:934] updating node { 192.168.49.2 8443 v1.32.0 docker true true} ...
I0413 21:58:47.869980    8940 kubeadm.go:946] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.32.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0413 21:58:47.884781    8940 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0413 21:58:48.079128    8940 cni.go:84] Creating CNI manager for ""
I0413 21:58:48.079128    8940 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0413 21:58:48.079128    8940 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0413 21:58:48.079128    8940 kubeadm.go:189] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.32.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0413 21:58:48.079128    8940 kubeadm.go:195] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    - name: "node-ip"
      value: "192.168.49.2"
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    - name: "enable-admission-plugins"
      value: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    - name: "allocate-node-cidrs"
      value: "true"
    - name: "leader-elect"
      value: "false"
scheduler:
  extraArgs:
    - name: "leader-elect"
      value: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      - name: "proxy-refresh-interval"
        value: "70000"
kubernetesVersion: v1.32.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0413 21:58:48.098352    8940 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.32.0
I0413 21:58:48.106242    8940 binaries.go:44] Found k8s binaries, skipping transfer
I0413 21:58:48.125648    8940 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0413 21:58:48.132260    8940 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0413 21:58:48.144278    8940 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0413 21:58:48.155768    8940 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2286 bytes)
I0413 21:58:48.179954    8940 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0413 21:58:48.182944    8940 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0413 21:58:48.208968    8940 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0413 21:58:48.305445    8940 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0413 21:58:48.316007    8940 certs.go:68] Setting up C:\Users\anima\.minikube\profiles\minikube for IP: 192.168.49.2
I0413 21:58:48.316007    8940 certs.go:194] generating shared ca certs ...
I0413 21:58:48.316007    8940 certs.go:226] acquiring lock for ca certs: {Name:mk368d428cd067a59dfc99ff80ff109126abd846 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0413 21:58:48.316528    8940 certs.go:235] skipping valid "minikubeCA" ca cert: C:\Users\anima\.minikube\ca.key
I0413 21:58:48.316528    8940 certs.go:235] skipping valid "proxyClientCA" ca cert: C:\Users\anima\.minikube\proxy-client-ca.key
I0413 21:58:48.316528    8940 certs.go:256] generating profile certs ...
I0413 21:58:48.317068    8940 certs.go:363] generating signed profile cert for "minikube-user": C:\Users\anima\.minikube\profiles\minikube\client.key
I0413 21:58:48.320185    8940 crypto.go:68] Generating cert C:\Users\anima\.minikube\profiles\minikube\client.crt with IP's: []
I0413 21:58:48.517569    8940 crypto.go:156] Writing cert to C:\Users\anima\.minikube\profiles\minikube\client.crt ...
I0413 21:58:48.517569    8940 lock.go:35] WriteFile acquiring C:\Users\anima\.minikube\profiles\minikube\client.crt: {Name:mk82d490508502f7beaf57b465db3acdc50571bf Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0413 21:58:48.517569    8940 crypto.go:164] Writing key to C:\Users\anima\.minikube\profiles\minikube\client.key ...
I0413 21:58:48.517569    8940 lock.go:35] WriteFile acquiring C:\Users\anima\.minikube\profiles\minikube\client.key: {Name:mk89050a3482ca8cfaa6ced9882bb0f59d90d796 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0413 21:58:48.518536    8940 certs.go:363] generating signed profile cert for "minikube": C:\Users\anima\.minikube\profiles\minikube\apiserver.key.7fb57e3c
I0413 21:58:48.518536    8940 crypto.go:68] Generating cert C:\Users\anima\.minikube\profiles\minikube\apiserver.crt.7fb57e3c with IP's: [10.96.0.1 127.0.0.1 10.0.0.1 192.168.49.2]
I0413 21:58:48.684035    8940 crypto.go:156] Writing cert to C:\Users\anima\.minikube\profiles\minikube\apiserver.crt.7fb57e3c ...
I0413 21:58:48.684035    8940 lock.go:35] WriteFile acquiring C:\Users\anima\.minikube\profiles\minikube\apiserver.crt.7fb57e3c: {Name:mk155cf1509990a88dd8e00c63819af1c28d13d9 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0413 21:58:48.684035    8940 crypto.go:164] Writing key to C:\Users\anima\.minikube\profiles\minikube\apiserver.key.7fb57e3c ...
I0413 21:58:48.684035    8940 lock.go:35] WriteFile acquiring C:\Users\anima\.minikube\profiles\minikube\apiserver.key.7fb57e3c: {Name:mk2bbe22d04e925bca2ebb062e14596a933f51d2 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0413 21:58:48.685036    8940 certs.go:381] copying C:\Users\anima\.minikube\profiles\minikube\apiserver.crt.7fb57e3c -> C:\Users\anima\.minikube\profiles\minikube\apiserver.crt
I0413 21:58:48.692216    8940 certs.go:385] copying C:\Users\anima\.minikube\profiles\minikube\apiserver.key.7fb57e3c -> C:\Users\anima\.minikube\profiles\minikube\apiserver.key
I0413 21:58:48.693217    8940 certs.go:363] generating signed profile cert for "aggregator": C:\Users\anima\.minikube\profiles\minikube\proxy-client.key
I0413 21:58:48.693217    8940 crypto.go:68] Generating cert C:\Users\anima\.minikube\profiles\minikube\proxy-client.crt with IP's: []
I0413 21:58:48.835515    8940 crypto.go:156] Writing cert to C:\Users\anima\.minikube\profiles\minikube\proxy-client.crt ...
I0413 21:58:48.835515    8940 lock.go:35] WriteFile acquiring C:\Users\anima\.minikube\profiles\minikube\proxy-client.crt: {Name:mkdff65059aa68964355c019e362f6aa762ce23d Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0413 21:58:48.835515    8940 crypto.go:164] Writing key to C:\Users\anima\.minikube\profiles\minikube\proxy-client.key ...
I0413 21:58:48.835515    8940 lock.go:35] WriteFile acquiring C:\Users\anima\.minikube\profiles\minikube\proxy-client.key: {Name:mk9228cdc3abcf9756eb9757de2235ae2eb11804 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0413 21:58:48.847587    8940 certs.go:484] found cert: C:\Users\anima\.minikube\certs\ca-key.pem (1679 bytes)
I0413 21:58:48.847587    8940 certs.go:484] found cert: C:\Users\anima\.minikube\certs\ca.pem (1086 bytes)
I0413 21:58:48.847587    8940 certs.go:484] found cert: C:\Users\anima\.minikube\certs\cert.pem (1131 bytes)
I0413 21:58:48.847587    8940 certs.go:484] found cert: C:\Users\anima\.minikube\certs\key.pem (1675 bytes)
I0413 21:58:48.848594    8940 ssh_runner.go:362] scp C:\Users\anima\.minikube\ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0413 21:58:48.865599    8940 ssh_runner.go:362] scp C:\Users\anima\.minikube\ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0413 21:58:48.881871    8940 ssh_runner.go:362] scp C:\Users\anima\.minikube\proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0413 21:58:48.897830    8940 ssh_runner.go:362] scp C:\Users\anima\.minikube\proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0413 21:58:48.913894    8940 ssh_runner.go:362] scp C:\Users\anima\.minikube\profiles\minikube\apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0413 21:58:48.929776    8940 ssh_runner.go:362] scp C:\Users\anima\.minikube\profiles\minikube\apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0413 21:58:48.945863    8940 ssh_runner.go:362] scp C:\Users\anima\.minikube\profiles\minikube\proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0413 21:58:48.962180    8940 ssh_runner.go:362] scp C:\Users\anima\.minikube\profiles\minikube\proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0413 21:58:48.978145    8940 ssh_runner.go:362] scp C:\Users\anima\.minikube\ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0413 21:58:48.997863    8940 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0413 21:58:49.011998    8940 ssh_runner.go:195] Run: openssl version
I0413 21:58:49.039864    8940 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0413 21:58:49.060988    8940 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0413 21:58:49.063667    8940 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Apr 10 17:47 /usr/share/ca-certificates/minikubeCA.pem
I0413 21:58:49.066255    8940 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0413 21:58:49.090348    8940 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0413 21:58:49.110200    8940 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0413 21:58:49.113197    8940 certs.go:399] 'apiserver-kubelet-client' cert doesn't exist, likely first start: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/certs/apiserver-kubelet-client.crt': No such file or directory
I0413 21:58:49.113719    8940 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:7800 CPUs:4 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\anima:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0413 21:58:49.128858    8940 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0413 21:58:49.162205    8940 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0413 21:58:49.188839    8940 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0413 21:58:49.196280    8940 kubeadm.go:214] ignoring SystemVerification for kubeadm because of docker driver
I0413 21:58:49.215359    8940 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0413 21:58:49.222217    8940 kubeadm.go:155] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I0413 21:58:49.222217    8940 kubeadm.go:157] found existing configuration files:

I0413 21:58:49.241539    8940 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0413 21:58:49.248311    8940 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/admin.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/admin.conf: No such file or directory
I0413 21:58:49.267562    8940 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/admin.conf
I0413 21:58:49.293367    8940 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0413 21:58:49.300561    8940 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/kubelet.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/kubelet.conf: No such file or directory
I0413 21:58:49.320883    8940 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/kubelet.conf
I0413 21:58:49.347866    8940 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0413 21:58:49.355701    8940 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/controller-manager.conf: No such file or directory
I0413 21:58:49.375040    8940 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0413 21:58:49.401032    8940 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0413 21:58:49.407581    8940 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/scheduler.conf: No such file or directory
I0413 21:58:49.426876    8940 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0413 21:58:49.433368    8940 ssh_runner.go:286] Start: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.32.0:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables"
I0413 21:58:49.474341    8940 kubeadm.go:310] 	[WARNING Swap]: swap is supported for cgroup v2 only. The kubelet must be properly configured to use swap. Please refer to https://kubernetes.io/docs/concepts/architecture/nodes/#swap-memory, or disable swap on the node
I0413 21:58:49.478998    8940 kubeadm.go:310] 	[WARNING SystemVerification]: cgroups v1 support is in maintenance mode, please migrate to cgroups v2
I0413 21:58:49.520246    8940 kubeadm.go:310] 	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
I0413 21:58:56.705783    8940 kubeadm.go:310] [init] Using Kubernetes version: v1.32.0
I0413 21:58:56.705783    8940 kubeadm.go:310] [preflight] Running pre-flight checks
I0413 21:58:56.705783    8940 kubeadm.go:310] [preflight] Pulling images required for setting up a Kubernetes cluster
I0413 21:58:56.705783    8940 kubeadm.go:310] [preflight] This might take a minute or two, depending on the speed of your internet connection
I0413 21:58:56.705783    8940 kubeadm.go:310] [preflight] You can also perform this action beforehand using 'kubeadm config images pull'
I0413 21:58:56.706305    8940 kubeadm.go:310] [certs] Using certificateDir folder "/var/lib/minikube/certs"
I0413 21:58:56.706822    8940 out.go:235]   - Generating certificates and keys ...
I0413 21:58:56.707862    8940 kubeadm.go:310] [certs] Using existing ca certificate authority
I0413 21:58:56.707862    8940 kubeadm.go:310] [certs] Using existing apiserver certificate and key on disk
I0413 21:58:56.707862    8940 kubeadm.go:310] [certs] Generating "apiserver-kubelet-client" certificate and key
I0413 21:58:56.707862    8940 kubeadm.go:310] [certs] Generating "front-proxy-ca" certificate and key
I0413 21:58:56.707862    8940 kubeadm.go:310] [certs] Generating "front-proxy-client" certificate and key
I0413 21:58:56.707862    8940 kubeadm.go:310] [certs] Generating "etcd/ca" certificate and key
I0413 21:58:56.707862    8940 kubeadm.go:310] [certs] Generating "etcd/server" certificate and key
I0413 21:58:56.708381    8940 kubeadm.go:310] [certs] etcd/server serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0413 21:58:56.708381    8940 kubeadm.go:310] [certs] Generating "etcd/peer" certificate and key
I0413 21:58:56.708381    8940 kubeadm.go:310] [certs] etcd/peer serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0413 21:58:56.708381    8940 kubeadm.go:310] [certs] Generating "etcd/healthcheck-client" certificate and key
I0413 21:58:56.708381    8940 kubeadm.go:310] [certs] Generating "apiserver-etcd-client" certificate and key
I0413 21:58:56.708381    8940 kubeadm.go:310] [certs] Generating "sa" key and public key
I0413 21:58:56.708381    8940 kubeadm.go:310] [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
I0413 21:58:56.708886    8940 kubeadm.go:310] [kubeconfig] Writing "admin.conf" kubeconfig file
I0413 21:58:56.708902    8940 kubeadm.go:310] [kubeconfig] Writing "super-admin.conf" kubeconfig file
I0413 21:58:56.708902    8940 kubeadm.go:310] [kubeconfig] Writing "kubelet.conf" kubeconfig file
I0413 21:58:56.708902    8940 kubeadm.go:310] [kubeconfig] Writing "controller-manager.conf" kubeconfig file
I0413 21:58:56.708902    8940 kubeadm.go:310] [kubeconfig] Writing "scheduler.conf" kubeconfig file
I0413 21:58:56.708902    8940 kubeadm.go:310] [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
I0413 21:58:56.708902    8940 kubeadm.go:310] [control-plane] Using manifest folder "/etc/kubernetes/manifests"
I0413 21:58:56.709934    8940 out.go:235]   - Booting up control plane ...
I0413 21:58:56.709934    8940 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-apiserver"
I0413 21:58:56.710451    8940 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-controller-manager"
I0413 21:58:56.710451    8940 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-scheduler"
I0413 21:58:56.710451    8940 kubeadm.go:310] [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
I0413 21:58:56.710451    8940 kubeadm.go:310] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
I0413 21:58:56.710451    8940 kubeadm.go:310] [kubelet-start] Starting the kubelet
I0413 21:58:56.710955    8940 kubeadm.go:310] [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests"
I0413 21:58:56.711056    8940 kubeadm.go:310] [kubelet-check] Waiting for a healthy kubelet at http://127.0.0.1:10248/healthz. This can take up to 4m0s
I0413 21:58:56.711056    8940 kubeadm.go:310] [kubelet-check] The kubelet is healthy after 1.001276627s
I0413 21:58:56.711056    8940 kubeadm.go:310] [api-check] Waiting for a healthy API server. This can take up to 4m0s
I0413 21:58:56.711056    8940 kubeadm.go:310] [api-check] The API server is healthy after 3.002307443s
I0413 21:58:56.711056    8940 kubeadm.go:310] [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
I0413 21:58:56.711056    8940 kubeadm.go:310] [kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
I0413 21:58:56.711559    8940 kubeadm.go:310] [upload-certs] Skipping phase. Please see --upload-certs
I0413 21:58:56.711575    8940 kubeadm.go:310] [mark-control-plane] Marking the node minikube as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
I0413 21:58:56.711575    8940 kubeadm.go:310] [bootstrap-token] Using token: xp33kf.4jze6er2msw2s5bi
I0413 21:58:56.712095    8940 out.go:235]   - Configuring RBAC rules ...
I0413 21:58:56.713128    8940 kubeadm.go:310] [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
I0413 21:58:56.713128    8940 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
I0413 21:58:56.713128    8940 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
I0413 21:58:56.713128    8940 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
I0413 21:58:56.713648    8940 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
I0413 21:58:56.713648    8940 kubeadm.go:310] [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
I0413 21:58:56.713648    8940 kubeadm.go:310] [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
I0413 21:58:56.713648    8940 kubeadm.go:310] [addons] Applied essential addon: CoreDNS
I0413 21:58:56.713648    8940 kubeadm.go:310] [addons] Applied essential addon: kube-proxy
I0413 21:58:56.713648    8940 kubeadm.go:310] 
I0413 21:58:56.713648    8940 kubeadm.go:310] Your Kubernetes control-plane has initialized successfully!
I0413 21:58:56.713648    8940 kubeadm.go:310] 
I0413 21:58:56.714182    8940 kubeadm.go:310] To start using your cluster, you need to run the following as a regular user:
I0413 21:58:56.714182    8940 kubeadm.go:310] 
I0413 21:58:56.714182    8940 kubeadm.go:310]   mkdir -p $HOME/.kube
I0413 21:58:56.714182    8940 kubeadm.go:310]   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
I0413 21:58:56.714182    8940 kubeadm.go:310]   sudo chown $(id -u):$(id -g) $HOME/.kube/config
I0413 21:58:56.714182    8940 kubeadm.go:310] 
I0413 21:58:56.714182    8940 kubeadm.go:310] Alternatively, if you are the root user, you can run:
I0413 21:58:56.714182    8940 kubeadm.go:310] 
I0413 21:58:56.714182    8940 kubeadm.go:310]   export KUBECONFIG=/etc/kubernetes/admin.conf
I0413 21:58:56.714182    8940 kubeadm.go:310] 
I0413 21:58:56.714182    8940 kubeadm.go:310] You should now deploy a pod network to the cluster.
I0413 21:58:56.714182    8940 kubeadm.go:310] Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
I0413 21:58:56.714182    8940 kubeadm.go:310]   https://kubernetes.io/docs/concepts/cluster-administration/addons/
I0413 21:58:56.714687    8940 kubeadm.go:310] 
I0413 21:58:56.714706    8940 kubeadm.go:310] You can now join any number of control-plane nodes by copying certificate authorities
I0413 21:58:56.714706    8940 kubeadm.go:310] and service account keys on each node and then running the following as root:
I0413 21:58:56.714706    8940 kubeadm.go:310] 
I0413 21:58:56.714706    8940 kubeadm.go:310]   kubeadm join control-plane.minikube.internal:8443 --token xp33kf.4jze6er2msw2s5bi \
I0413 21:58:56.714706    8940 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:1c99ee2ca8c00655fd2cef7a7c51211524eac00dec1d317725594c0a2ab35803 \
I0413 21:58:56.714706    8940 kubeadm.go:310] 	--control-plane 
I0413 21:58:56.714706    8940 kubeadm.go:310] 
I0413 21:58:56.714706    8940 kubeadm.go:310] Then you can join any number of worker nodes by running the following on each as root:
I0413 21:58:56.714706    8940 kubeadm.go:310] 
I0413 21:58:56.715225    8940 kubeadm.go:310] kubeadm join control-plane.minikube.internal:8443 --token xp33kf.4jze6er2msw2s5bi \
I0413 21:58:56.715225    8940 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:1c99ee2ca8c00655fd2cef7a7c51211524eac00dec1d317725594c0a2ab35803 
I0413 21:58:56.715225    8940 cni.go:84] Creating CNI manager for ""
I0413 21:58:56.715225    8940 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0413 21:58:56.715746    8940 out.go:177] * Configuring bridge CNI (Container Networking Interface) ...
I0413 21:58:56.735439    8940 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I0413 21:58:56.742641    8940 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (496 bytes)
I0413 21:58:56.754676    8940 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0413 21:58:56.761541    8940 ops.go:34] apiserver oom_adj: -16
I0413 21:58:56.776406    8940 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.32.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig label --overwrite nodes minikube minikube.k8s.io/updated_at=2025_04_13T21_58_56_0700 minikube.k8s.io/version=v1.35.0 minikube.k8s.io/commit=dd5d320e41b5451cdf3c01891bc4e13d189586ed-dirty minikube.k8s.io/name=minikube minikube.k8s.io/primary=true
I0413 21:58:56.776912    8940 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.32.0/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig
I0413 21:58:56.824329    8940 kubeadm.go:1113] duration metric: took 69.6532ms to wait for elevateKubeSystemPrivileges
I0413 21:58:56.841246    8940 kubeadm.go:394] duration metric: took 7.727527s to StartCluster
I0413 21:58:56.841246    8940 settings.go:142] acquiring lock: {Name:mk7dbbba560cabb46506a887b00711f96fadafae Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0413 21:58:56.841246    8940 settings.go:150] Updating kubeconfig:  C:\Users\anima\.kube\config
I0413 21:58:56.842289    8940 lock.go:35] WriteFile acquiring C:\Users\anima\.kube\config: {Name:mk41156f71b08d4902eeda963a0286b5dfab653a Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0413 21:58:56.842289    8940 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0413 21:58:56.842289    8940 addons.go:511] enable addons start: toEnable=map[ambassador:false amd-gpu-device-plugin:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I0413 21:58:56.842814    8940 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0413 21:58:56.842814    8940 addons.go:238] Setting addon storage-provisioner=true in "minikube"
I0413 21:58:56.842814    8940 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.32.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0413 21:58:56.842814    8940 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0413 21:58:56.842814    8940 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0413 21:58:56.842814    8940 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0413 21:58:56.842814    8940 host.go:66] Checking if "minikube" exists ...
I0413 21:58:56.843338    8940 out.go:177] * Verifying Kubernetes components...
I0413 21:58:56.868129    8940 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0413 21:58:56.889259    8940 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0413 21:58:56.889813    8940 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0413 21:58:56.933327    8940 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.32.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed -e '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           192.168.65.254 host.minikube.internal\n           fallthrough\n        }' -e '/^        errors *$/i \        log' | sudo /var/lib/minikube/binaries/v1.32.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -"
I0413 21:58:56.957188    8940 addons.go:238] Setting addon default-storageclass=true in "minikube"
I0413 21:58:56.957188    8940 host.go:66] Checking if "minikube" exists ...
I0413 21:58:56.958774    8940 out.go:177]   - Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0413 21:58:56.959865    8940 addons.go:435] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0413 21:58:56.959865    8940 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0413 21:58:56.986227    8940 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0413 21:58:57.006532    8940 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0413 21:58:57.023346    8940 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0413 21:58:57.029213    8940 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:53618 SSHKeyPath:C:\Users\anima\.minikube\machines\minikube\id_rsa Username:docker}
I0413 21:58:57.046847    8940 addons.go:435] installing /etc/kubernetes/addons/storageclass.yaml
I0413 21:58:57.046847    8940 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0413 21:58:57.062808    8940 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0413 21:58:57.071836    8940 start.go:971] {"host.minikube.internal": 192.168.65.254} host record injected into CoreDNS's ConfigMap
I0413 21:58:57.086951    8940 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0413 21:58:57.097250    8940 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:53618 SSHKeyPath:C:\Users\anima\.minikube\machines\minikube\id_rsa Username:docker}
I0413 21:58:57.121164    8940 api_server.go:52] waiting for apiserver process to appear ...
I0413 21:58:57.143626    8940 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0413 21:58:57.151896    8940 api_server.go:72] duration metric: took 309.103ms to wait for apiserver process to appear ...
I0413 21:58:57.151896    8940 api_server.go:88] waiting for apiserver healthz status ...
I0413 21:58:57.152413    8940 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:53617/healthz ...
I0413 21:58:57.157111    8940 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0413 21:58:57.157111    8940 api_server.go:279] https://127.0.0.1:53617/healthz returned 200:
ok
I0413 21:58:57.158681    8940 api_server.go:141] control plane version: v1.32.0
I0413 21:58:57.158681    8940 api_server.go:131] duration metric: took 6.7855ms to wait for apiserver health ...
I0413 21:58:57.158681    8940 system_pods.go:43] waiting for kube-system pods to appear ...
I0413 21:58:57.173264    8940 system_pods.go:59] 4 kube-system pods found
I0413 21:58:57.173264    8940 system_pods.go:61] "etcd-minikube" [3797bf35-c6e6-4615-b7e7-07afb1be3406] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0413 21:58:57.173264    8940 system_pods.go:61] "kube-apiserver-minikube" [4e1e2e0b-f283-46e5-ba9e-d91a98638d70] Running
I0413 21:58:57.173264    8940 system_pods.go:61] "kube-controller-manager-minikube" [cfd53c9b-5281-483c-891e-2aae194b2eb9] Running
I0413 21:58:57.173264    8940 system_pods.go:61] "kube-scheduler-minikube" [7fbd48a6-05ed-4e45-a917-af30381d452f] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0413 21:58:57.173264    8940 system_pods.go:74] duration metric: took 14.5825ms to wait for pod list to return data ...
I0413 21:58:57.173264    8940 kubeadm.go:582] duration metric: took 330.471ms to wait for: map[apiserver:true system_pods:true]
I0413 21:58:57.173264    8940 node_conditions.go:102] verifying NodePressure condition ...
I0413 21:58:57.176304    8940 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0413 21:58:57.176304    8940 node_conditions.go:123] node cpu capacity is 12
I0413 21:58:57.176304    8940 node_conditions.go:105] duration metric: took 3.0402ms to run NodePressure ...
I0413 21:58:57.176304    8940 start.go:241] waiting for startup goroutines ...
I0413 21:58:57.220454    8940 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0413 21:58:57.358797    8940 out.go:177] * Enabled addons: storage-provisioner, default-storageclass
I0413 21:58:57.359835    8940 addons.go:514] duration metric: took 517.5453ms for enable addons: enabled=[storage-provisioner default-storageclass]
I0413 21:58:57.576221    8940 kapi.go:214] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I0413 21:58:57.576243    8940 start.go:246] waiting for cluster config update ...
I0413 21:58:57.576243    8940 start.go:255] writing updated cluster config ...
I0413 21:58:57.591873    8940 ssh_runner.go:195] Run: rm -f paused
I0413 21:58:57.935305    8940 start.go:600] kubectl: 1.32.0, cluster: 1.32.0 (minor skew: 0)
I0413 21:58:57.935828    8940 out.go:177] * Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Apr 14 04:58:43 minikube dockerd[806]: time="2025-04-14T04:58:43.092341041Z" level=info msg="stopping event stream following graceful shutdown" error="context canceled" module=libcontainerd namespace=plugins.moby
Apr 14 04:58:43 minikube systemd[1]: docker.service: Deactivated successfully.
Apr 14 04:58:43 minikube systemd[1]: Stopped Docker Application Container Engine.
Apr 14 04:58:43 minikube systemd[1]: Starting Docker Application Container Engine...
Apr 14 04:58:43 minikube dockerd[1122]: time="2025-04-14T04:58:43.205743708Z" level=info msg="Starting up"
Apr 14 04:58:43 minikube dockerd[1122]: time="2025-04-14T04:58:43.206565807Z" level=info msg="OTEL tracing is not configured, using no-op tracer provider"
Apr 14 04:58:42 minikube dockerd[1122]: time="2025-04-14T04:58:42.145670423Z" level=info msg="[graphdriver] using prior storage driver: overlay2"
Apr 14 04:58:42 minikube dockerd[1122]: time="2025-04-14T04:58:42.148478751Z" level=info msg="Loading containers: start."
Apr 14 04:58:42 minikube dockerd[1122]: time="2025-04-14T04:58:42.591863594Z" level=info msg="Processing signal 'terminated'"
Apr 14 04:58:43 minikube dockerd[1122]: time="2025-04-14T04:58:43.669057285Z" level=info msg="Default bridge (docker0) is assigned with an IP address 172.17.0.0/16. Daemon option --bip can be used to set a preferred IP address"
Apr 14 04:58:43 minikube dockerd[1122]: time="2025-04-14T04:58:43.972285403Z" level=info msg="Loading containers: done."
Apr 14 04:58:43 minikube dockerd[1122]: time="2025-04-14T04:58:43.987329989Z" level=warning msg="WARNING: No blkio throttle.read_bps_device support"
Apr 14 04:58:43 minikube dockerd[1122]: time="2025-04-14T04:58:43.987366771Z" level=warning msg="WARNING: No blkio throttle.write_bps_device support"
Apr 14 04:58:43 minikube dockerd[1122]: time="2025-04-14T04:58:43.987375544Z" level=warning msg="WARNING: No blkio throttle.read_iops_device support"
Apr 14 04:58:43 minikube dockerd[1122]: time="2025-04-14T04:58:43.987379243Z" level=warning msg="WARNING: No blkio throttle.write_iops_device support"
Apr 14 04:58:43 minikube dockerd[1122]: time="2025-04-14T04:58:43.987396155Z" level=info msg="Docker daemon" commit=c710b88 containerd-snapshotter=false storage-driver=overlay2 version=27.4.1
Apr 14 04:58:43 minikube dockerd[1122]: time="2025-04-14T04:58:43.987427758Z" level=info msg="Daemon has completed initialization"
Apr 14 04:58:44 minikube dockerd[1122]: time="2025-04-14T04:58:44.017978713Z" level=info msg="API listen on /var/run/docker.sock"
Apr 14 04:58:44 minikube dockerd[1122]: time="2025-04-14T04:58:44.017984949Z" level=info msg="API listen on [::]:2376"
Apr 14 04:58:44 minikube dockerd[1122]: time="2025-04-14T04:58:44.019161868Z" level=info msg="stopping event stream following graceful shutdown" error="<nil>" module=libcontainerd namespace=moby
Apr 14 04:58:44 minikube dockerd[1122]: time="2025-04-14T04:58:44.019728501Z" level=info msg="Daemon shutdown complete"
Apr 14 04:58:44 minikube systemd[1]: docker.service: Deactivated successfully.
Apr 14 04:58:44 minikube systemd[1]: Stopped Docker Application Container Engine.
Apr 14 04:58:44 minikube systemd[1]: Starting Docker Application Container Engine...
Apr 14 04:58:44 minikube dockerd[1401]: time="2025-04-14T04:58:44.047951514Z" level=info msg="Starting up"
Apr 14 04:58:44 minikube dockerd[1401]: time="2025-04-14T04:58:44.048851828Z" level=info msg="OTEL tracing is not configured, using no-op tracer provider"
Apr 14 04:58:44 minikube dockerd[1401]: time="2025-04-14T04:58:44.062415198Z" level=info msg="[graphdriver] trying configured driver: overlay2"
Apr 14 04:58:44 minikube dockerd[1401]: time="2025-04-14T04:58:44.071439159Z" level=info msg="Loading containers: start."
Apr 14 04:58:45 minikube dockerd[1401]: time="2025-04-14T04:58:45.499784241Z" level=info msg="Default bridge (docker0) is assigned with an IP address 172.17.0.0/16. Daemon option --bip can be used to set a preferred IP address"
Apr 14 04:58:45 minikube dockerd[1401]: time="2025-04-14T04:58:45.782450788Z" level=info msg="Loading containers: done."
Apr 14 04:58:45 minikube dockerd[1401]: time="2025-04-14T04:58:45.794116818Z" level=warning msg="WARNING: No blkio throttle.read_bps_device support"
Apr 14 04:58:45 minikube dockerd[1401]: time="2025-04-14T04:58:45.794148738Z" level=warning msg="WARNING: No blkio throttle.write_bps_device support"
Apr 14 04:58:45 minikube dockerd[1401]: time="2025-04-14T04:58:45.794156349Z" level=warning msg="WARNING: No blkio throttle.read_iops_device support"
Apr 14 04:58:45 minikube dockerd[1401]: time="2025-04-14T04:58:45.794160365Z" level=warning msg="WARNING: No blkio throttle.write_iops_device support"
Apr 14 04:58:45 minikube dockerd[1401]: time="2025-04-14T04:58:45.794175268Z" level=info msg="Docker daemon" commit=c710b88 containerd-snapshotter=false storage-driver=overlay2 version=27.4.1
Apr 14 04:58:45 minikube dockerd[1401]: time="2025-04-14T04:58:45.794206342Z" level=info msg="Daemon has completed initialization"
Apr 14 04:58:45 minikube dockerd[1401]: time="2025-04-14T04:58:45.821742335Z" level=info msg="API listen on /var/run/docker.sock"
Apr 14 04:58:45 minikube dockerd[1401]: time="2025-04-14T04:58:45.821756392Z" level=info msg="API listen on [::]:2376"
Apr 14 04:58:45 minikube systemd[1]: Started Docker Application Container Engine.
Apr 14 04:58:46 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
Apr 14 04:58:46 minikube cri-dockerd[1682]: time="2025-04-14T04:58:46Z" level=info msg="Starting cri-dockerd dev (HEAD)"
Apr 14 04:58:46 minikube cri-dockerd[1682]: time="2025-04-14T04:58:46Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
Apr 14 04:58:46 minikube cri-dockerd[1682]: time="2025-04-14T04:58:46Z" level=info msg="Start docker client with request timeout 0s"
Apr 14 04:58:46 minikube cri-dockerd[1682]: time="2025-04-14T04:58:46Z" level=info msg="Hairpin mode is set to hairpin-veth"
Apr 14 04:58:46 minikube cri-dockerd[1682]: time="2025-04-14T04:58:46Z" level=info msg="Loaded network plugin cni"
Apr 14 04:58:46 minikube cri-dockerd[1682]: time="2025-04-14T04:58:46Z" level=info msg="Docker cri networking managed by network plugin cni"
Apr 14 04:58:46 minikube cri-dockerd[1682]: time="2025-04-14T04:58:46Z" level=info msg="Setting cgroupDriver cgroupfs"
Apr 14 04:58:46 minikube cri-dockerd[1682]: time="2025-04-14T04:58:46Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
Apr 14 04:58:46 minikube cri-dockerd[1682]: time="2025-04-14T04:58:46Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Apr 14 04:58:46 minikube cri-dockerd[1682]: time="2025-04-14T04:58:46Z" level=info msg="Start cri-dockerd grpc backend"
Apr 14 04:58:46 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Apr 14 04:58:52 minikube cri-dockerd[1682]: time="2025-04-14T04:58:52Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/a3a02f3256b0738e1c1e91c29f9d5cb9bc45c31718c1d2e9ede8edeb7aa16d69/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Apr 14 04:58:52 minikube cri-dockerd[1682]: time="2025-04-14T04:58:52Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/993cbe2766e41d9c73c05af19980306a71913feea019699643f1d8cd8558b5f1/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Apr 14 04:58:52 minikube cri-dockerd[1682]: time="2025-04-14T04:58:52Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/a6fd3b889bfe0a47bcc237874cd0c9939fb33ee56118f44fa9ba110283e912db/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Apr 14 04:58:52 minikube cri-dockerd[1682]: time="2025-04-14T04:58:52Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/0c25b9c409b99a8e42e82870c822b98346af1dd1882dbe314d9c29b6ca1b52a7/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Apr 14 04:59:01 minikube cri-dockerd[1682]: time="2025-04-14T04:59:01Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/fadbdf79c0674ce371845c9f2d8bc004d40c05ba57eaa38a342bb2e306764ff1/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Apr 14 04:59:01 minikube cri-dockerd[1682]: time="2025-04-14T04:59:01Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/8c402b87f0bb9bfcbd854fbf4f81f7cbdfc95a1d00bb98084e5af11718636ece/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Apr 14 04:59:02 minikube cri-dockerd[1682]: time="2025-04-14T04:59:02Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/fb265a804c5cb7a400d70a56a189b5816223cde1ce10278a5fa5ce5a0d1265cd/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Apr 14 04:59:06 minikube cri-dockerd[1682]: time="2025-04-14T04:59:06Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Apr 14 04:59:22 minikube dockerd[1401]: time="2025-04-14T04:59:22.652196247Z" level=info msg="ignoring event" container=915885efdf5eb69765e011f317a51c4d98c845356a4700ae459ff135327e2c19 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"


==> container status <==
CONTAINER           IMAGE               CREATED              STATE               NAME                      ATTEMPT             POD ID              POD
87ef88fcc7a11       6e38f40d628db       About a minute ago   Running             storage-provisioner       1                   fb265a804c5cb       storage-provisioner
915885efdf5eb       6e38f40d628db       About a minute ago   Exited              storage-provisioner       0                   fb265a804c5cb       storage-provisioner
ae2a3ae3cb782       c69fa2e9cbf5f       2 minutes ago        Running             coredns                   0                   8c402b87f0bb9       coredns-668d6bf9bc-9gwws
9e801094a2608       040f9f8aac8cd       2 minutes ago        Running             kube-proxy                0                   fadbdf79c0674       kube-proxy-fcvbd
60dea401fc0c4       c2e17b8d0f4a3       2 minutes ago        Running             kube-apiserver            0                   0c25b9c409b99       kube-apiserver-minikube
e0e837428a79e       a389e107f4ff1       2 minutes ago        Running             kube-scheduler            0                   a6fd3b889bfe0       kube-scheduler-minikube
2827c9f3458f8       8cab3d2a8bd0f       2 minutes ago        Running             kube-controller-manager   0                   a3a02f3256b07       kube-controller-manager-minikube
26a963f84edde       a9e7e6b294baf       2 minutes ago        Running             etcd                      0                   993cbe2766e41       etcd-minikube


==> coredns [ae2a3ae3cb78] <==
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = e7e8a6c4578bf29b9f453cb54ade3fb14671793481527b7435e35119b25e84eb3a79242b1f470199f8605ace441674db8f1b6715b77448c20dde63e2dc5d2169
CoreDNS-1.11.3
linux/amd64, go1.21.11, a6338e9
[INFO] 127.0.0.1:51121 - 29797 "HINFO IN 4970013435360294327.4541263989156923274. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.048498272s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: Trace[1758443024]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (14-Apr-2025 04:59:02.093) (total time: 22255ms):
Trace[1758443024]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 22255ms (04:59:22.581)
Trace[1758443024]: [22.255945036s] [22.255945036s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: Trace[1175674819]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (14-Apr-2025 04:59:02.094) (total time: 22254ms):
Trace[1175674819]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 22254ms (04:59:22.581)
Trace[1175674819]: [22.254989109s] [22.254989109s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: Trace[948098230]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (14-Apr-2025 04:59:02.093) (total time: 22256ms):
Trace[948098230]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 22255ms (04:59:22.581)
Trace[948098230]: [22.25614051s] [22.25614051s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/ready: Still waiting on: "kubernetes"


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=dd5d320e41b5451cdf3c01891bc4e13d189586ed-dirty
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2025_04_13T21_58_56_0700
                    minikube.k8s.io/version=v1.35.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Mon, 14 Apr 2025 04:58:53 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Mon, 14 Apr 2025 05:01:01 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Mon, 14 Apr 2025 04:59:06 +0000   Mon, 14 Apr 2025 04:58:53 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Mon, 14 Apr 2025 04:59:06 +0000   Mon, 14 Apr 2025 04:58:53 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Mon, 14 Apr 2025 04:59:06 +0000   Mon, 14 Apr 2025 04:58:53 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Mon, 14 Apr 2025 04:59:06 +0000   Mon, 14 Apr 2025 04:58:53 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                12
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             8027312Ki
  pods:               110
Allocatable:
  cpu:                12
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             8027312Ki
  pods:               110
System Info:
  Machine ID:                 8427162597844d07bd798363016992c6
  System UUID:                8427162597844d07bd798363016992c6
  Boot ID:                    60d3f1b7-944e-4091-af59-7d163292a2ab
  Kernel Version:             5.15.167.4-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://27.4.1
  Kubelet Version:            v1.32.0
  Kube-Proxy Version:         v1.32.0
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (7 in total)
  Namespace                   Name                                CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                ------------  ----------  ---------------  -------------  ---
  kube-system                 coredns-668d6bf9bc-9gwws            100m (0%)     0 (0%)      70Mi (0%)        170Mi (2%)     2m
  kube-system                 etcd-minikube                       100m (0%)     0 (0%)      100Mi (1%)       0 (0%)         2m6s
  kube-system                 kube-apiserver-minikube             250m (2%)     0 (0%)      0 (0%)           0 (0%)         2m6s
  kube-system                 kube-controller-manager-minikube    200m (1%)     0 (0%)      0 (0%)           0 (0%)         2m7s
  kube-system                 kube-proxy-fcvbd                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         2m
  kube-system                 kube-scheduler-minikube             100m (0%)     0 (0%)      0 (0%)           0 (0%)         2m7s
  kube-system                 storage-provisioner                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         2m4s
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (6%)   0 (0%)
  memory             170Mi (2%)  170Mi (2%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:
  Type     Reason                             Age                    From             Message
  ----     ------                             ----                   ----             -------
  Normal   Starting                           119s                   kube-proxy       
  Normal   NodeHasSufficientMemory            2m10s (x8 over 2m10s)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure              2m10s (x8 over 2m10s)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID               2m10s (x7 over 2m10s)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced            2m10s                  kubelet          Updated Node Allocatable limit across pods
  Normal   Starting                           2m6s                   kubelet          Starting kubelet.
  Warning  CgroupV1                           2m6s                   kubelet          cgroup v1 support is in maintenance mode, please migrate to cgroup v2
  Normal   NodeAllocatableEnforced            2m6s                   kubelet          Updated Node Allocatable limit across pods
  Warning  PossibleMemoryBackedVolumesOnDisk  2m6s                   kubelet          The tmpfs noswap option is not supported. Memory-backed volumes (e.g. secrets, emptyDirs, etc.) might be swapped to disk and should no longer be considered secure.
  Normal   NodeHasSufficientMemory            2m5s                   kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure              2m5s                   kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID               2m5s                   kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   RegisteredNode                     2m1s                   node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[  +0.000337] FS-Cache: N-cookie c=0000001f [p=00000002 fl=2 nc=0 na=1]
[  +0.000457] FS-Cache: N-cookie d=00000000ec2710d1{9P.session} n=0000000042e9d2b5
[  +0.000603] FS-Cache: N-key=[10] '34323934393337373237'
[  +0.122029] /sbin/ldconfig.real: 
[  +0.000004] /usr/lib/wsl/lib/libcuda.so.1 is not a symbolic link

[  +0.013720] misc dxg: dxgk: dxgkio_is_feature_enabled: Ioctl failed: -22
[  +0.001195] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000573] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000395] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000551] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000602] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.002469] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000660] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000641] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000586] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.329796] Failed to connect to bus: No such file or directory
[Apr14 01:39] Failed to connect to bus: No such file or directory
[  +0.480193] systemd-journald[58]: File /var/log/journal/f6fb6d6e5dc84df18811fd0a5841d893/system.journal corrupted or uncleanly shut down, renaming and replacing.
[  +0.676781] systemd-journald[58]: Failed to read journal file /var/log/journal/f6fb6d6e5dc84df18811fd0a5841d893/user-1000.journal for rotation, trying to move it out of the way: Device or resource busy
[  +0.360550] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000007] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.196053] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000724] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000472] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000450] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000463] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000407] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000416] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000961] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000600] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000011] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000451] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000375] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000309] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000366] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000346] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000399] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000326] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000319] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000349] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000312] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000330] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000759] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000475] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.002641] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000557] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000557] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000565] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000472] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.439023] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.007564] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.148861] WSL (177) ERROR: CheckConnection: getaddrinfo() failed: -5
[  +0.430163] new mount options do not match the existing superblock, will be ignored
[  +0.001743] netlink: 'init': attribute type 4 has an invalid length.
[Apr14 02:13] WSL (1 - init(docker-desktop)) WARNING: /usr/share/zoneinfo/America/Los_Angeles not found. Is the tzdata package installed?
[  +0.000377] WSL (1 - init(docker-desktop)) WARNING: /usr/share/zoneinfo/America/Los_Angeles not found. Is the tzdata package installed?
[Apr14 04:16] WSL (1 - init(docker-desktop)) WARNING: /usr/share/zoneinfo/America/Los_Angeles not found. Is the tzdata package installed?
[Apr14 04:53] tmpfs: Unknown parameter 'noswap'
[Apr14 04:54] tmpfs: Unknown parameter 'noswap'


==> etcd [26a963f84edd] <==
{"level":"warn","ts":"2025-04-14T04:58:52.337728Z","caller":"embed/config.go:689","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2025-04-14T04:58:52.337828Z","caller":"etcdmain/etcd.go:73","msg":"Running: ","args":["etcd","--advertise-client-urls=https://192.168.49.2:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--experimental-initial-corrupt-check=true","--experimental-watch-progress-notify-interval=5s","--initial-advertise-peer-urls=https://192.168.49.2:2380","--initial-cluster=minikube=https://192.168.49.2:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://192.168.49.2:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://192.168.49.2:2380","--name=minikube","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"warn","ts":"2025-04-14T04:58:52.338151Z","caller":"embed/config.go:689","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2025-04-14T04:58:52.338196Z","caller":"embed/etcd.go:128","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2025-04-14T04:58:52.338235Z","caller":"embed/etcd.go:497","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2025-04-14T04:58:52.338578Z","caller":"embed/etcd.go:136","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"]}
{"level":"info","ts":"2025-04-14T04:58:52.338693Z","caller":"embed/etcd.go:311","msg":"starting an etcd server","etcd-version":"3.5.16","git-sha":"f20bbad","go-version":"go1.22.7","go-os":"linux","go-arch":"amd64","max-cpu-set":12,"max-cpu-available":12,"member-initialized":false,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"minikube=https://192.168.49.2:2380","initial-cluster-state":"new","initial-cluster-token":"etcd-cluster","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2025-04-14T04:58:52.341723Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"2.559416ms"}
{"level":"info","ts":"2025-04-14T04:58:52.349913Z","caller":"etcdserver/raft.go:505","msg":"starting local member","local-member-id":"aec36adc501070cc","cluster-id":"fa54960ea34d58be"}
{"level":"info","ts":"2025-04-14T04:58:52.349989Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=()"}
{"level":"info","ts":"2025-04-14T04:58:52.350026Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 0"}
{"level":"info","ts":"2025-04-14T04:58:52.350035Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [], term: 0, commit: 0, applied: 0, lastindex: 0, lastterm: 0]"}
{"level":"info","ts":"2025-04-14T04:58:52.350042Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 1"}
{"level":"info","ts":"2025-04-14T04:58:52.350074Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"warn","ts":"2025-04-14T04:58:52.354803Z","caller":"auth/store.go:1241","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2025-04-14T04:58:52.357022Z","caller":"mvcc/kvstore.go:423","msg":"kvstore restored","current-rev":1}
{"level":"info","ts":"2025-04-14T04:58:52.358672Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2025-04-14T04:58:52.360532Z","caller":"etcdserver/server.go:873","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.16","cluster-version":"to_be_decided"}
{"level":"info","ts":"2025-04-14T04:58:52.360889Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-04-14T04:58:52.361701Z","caller":"etcdserver/server.go:757","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"aec36adc501070cc","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2025-04-14T04:58:52.361822Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2025-04-14T04:58:52.361856Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2025-04-14T04:58:52.361865Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2025-04-14T04:58:52.362422Z","caller":"embed/etcd.go:729","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2025-04-14T04:58:52.362531Z","caller":"embed/etcd.go:600","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-04-14T04:58:52.362563Z","caller":"embed/etcd.go:572","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-04-14T04:58:52.362674Z","caller":"embed/etcd.go:280","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2025-04-14T04:58:52.362699Z","caller":"embed/etcd.go:871","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2025-04-14T04:58:52.363031Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2025-04-14T04:58:52.363153Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","added-peer-id":"aec36adc501070cc","added-peer-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2025-04-14T04:58:52.850336Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 1"}
{"level":"info","ts":"2025-04-14T04:58:52.850379Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 1"}
{"level":"info","ts":"2025-04-14T04:58:52.850393Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 1"}
{"level":"info","ts":"2025-04-14T04:58:52.850402Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 2"}
{"level":"info","ts":"2025-04-14T04:58:52.850413Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 2"}
{"level":"info","ts":"2025-04-14T04:58:52.850420Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 2"}
{"level":"info","ts":"2025-04-14T04:58:52.850425Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 2"}
{"level":"info","ts":"2025-04-14T04:58:52.855266Z","caller":"etcdserver/server.go:2140","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2025-04-14T04:58:52.855284Z","caller":"etcdserver/server.go:2651","msg":"setting up initial cluster version using v2 API","cluster-version":"3.5"}
{"level":"info","ts":"2025-04-14T04:58:52.855302Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-04-14T04:58:52.855343Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-04-14T04:58:52.855451Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2025-04-14T04:58:52.855508Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2025-04-14T04:58:52.856247Z","caller":"membership/cluster.go:584","msg":"set initial cluster version","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","cluster-version":"3.5"}
{"level":"info","ts":"2025-04-14T04:58:52.856289Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-04-14T04:58:52.856342Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2025-04-14T04:58:52.856378Z","caller":"etcdserver/server.go:2675","msg":"cluster version is updated","cluster-version":"3.5"}
{"level":"info","ts":"2025-04-14T04:58:52.856905Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-04-14T04:58:52.857524Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2025-04-14T04:58:52.857730Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}


==> kernel <==
 05:01:01 up  3:22,  0 users,  load average: 0.20, 0.32, 0.27
Linux minikube 5.15.167.4-microsoft-standard-WSL2 #1 SMP Tue Nov 5 00:21:55 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.5 LTS"


==> kube-apiserver [60dea401fc0c] <==
I0414 04:58:53.484125       1 local_available_controller.go:156] Starting LocalAvailability controller
I0414 04:58:53.484135       1 cache.go:32] Waiting for caches to sync for LocalAvailability controller
I0414 04:58:53.484154       1 aggregator.go:169] waiting for initial CRD sync...
I0414 04:58:53.484204       1 system_namespaces_controller.go:66] Starting system namespaces controller
I0414 04:58:53.484128       1 shared_informer.go:313] Waiting for caches to sync for configmaps
I0414 04:58:53.484254       1 crdregistration_controller.go:114] Starting crd-autoregister controller
I0414 04:58:53.484260       1 shared_informer.go:313] Waiting for caches to sync for crd-autoregister
I0414 04:58:53.484303       1 controller.go:80] Starting OpenAPI V3 AggregationController
I0414 04:58:53.498068       1 controller.go:78] Starting OpenAPI AggregationController
I0414 04:58:53.498130       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0414 04:58:53.498152       1 apiservice_controller.go:100] Starting APIServiceRegistrationController
I0414 04:58:53.498156       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0414 04:58:53.511410       1 remote_available_controller.go:411] Starting RemoteAvailability controller
I0414 04:58:53.511441       1 cache.go:32] Waiting for caches to sync for RemoteAvailability controller
I0414 04:58:53.511589       1 customresource_discovery_controller.go:292] Starting DiscoveryController
I0414 04:58:53.511676       1 cluster_authentication_trust_controller.go:462] Starting cluster_authentication_trust_controller controller
I0414 04:58:53.511690       1 shared_informer.go:313] Waiting for caches to sync for cluster_authentication_trust_controller
I0414 04:58:53.511786       1 controller.go:142] Starting OpenAPI controller
I0414 04:58:53.511815       1 controller.go:90] Starting OpenAPI V3 controller
I0414 04:58:53.511835       1 naming_controller.go:294] Starting NamingConditionController
I0414 04:58:53.511848       1 establishing_controller.go:81] Starting EstablishingController
I0414 04:58:53.511861       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I0414 04:58:53.511874       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0414 04:58:53.511877       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0414 04:58:53.511886       1 crd_finalizer.go:269] Starting CRDFinalizer
I0414 04:58:53.511933       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0414 04:58:53.564090       1 shared_informer.go:320] Caches are synced for node_authorizer
I0414 04:58:53.569807       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I0414 04:58:53.569835       1 policy_source.go:240] refreshing policies
E0414 04:58:53.578727       1 controller.go:145] "Failed to ensure lease exists, will retry" err="namespaces \"kube-system\" not found" interval="200ms"
I0414 04:58:53.584005       1 apf_controller.go:382] Running API Priority and Fairness config worker
I0414 04:58:53.584039       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I0414 04:58:53.584216       1 cache.go:39] Caches are synced for LocalAvailability controller
I0414 04:58:53.584293       1 shared_informer.go:320] Caches are synced for crd-autoregister
I0414 04:58:53.584300       1 shared_informer.go:320] Caches are synced for configmaps
I0414 04:58:53.584334       1 aggregator.go:171] initial CRD sync complete...
I0414 04:58:53.584367       1 autoregister_controller.go:144] Starting autoregister controller
I0414 04:58:53.584373       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0414 04:58:53.584379       1 cache.go:39] Caches are synced for autoregister controller
I0414 04:58:53.585333       1 controller.go:615] quota admission added evaluator for: namespaces
I0414 04:58:53.598256       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0414 04:58:53.607328       1 handler_discovery.go:451] Starting ResourceDiscoveryManager
I0414 04:58:53.612106       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
I0414 04:58:53.612159       1 cache.go:39] Caches are synced for RemoteAvailability controller
I0414 04:58:53.781575       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
I0414 04:58:54.491689       1 storage_scheduling.go:95] created PriorityClass system-node-critical with value 2000001000
I0414 04:58:54.494813       1 storage_scheduling.go:95] created PriorityClass system-cluster-critical with value 2000000000
I0414 04:58:54.494837       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0414 04:58:54.897749       1 controller.go:615] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0414 04:58:54.933134       1 controller.go:615] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0414 04:58:54.993694       1 alloc.go:330] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W0414 04:58:54.998863       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I0414 04:58:54.999611       1 controller.go:615] quota admission added evaluator for: endpoints
I0414 04:58:55.003213       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0414 04:58:55.534449       1 controller.go:615] quota admission added evaluator for: serviceaccounts
I0414 04:58:55.864281       1 controller.go:615] quota admission added evaluator for: deployments.apps
I0414 04:58:55.879942       1 alloc.go:330] "allocated clusterIPs" service="kube-system/kube-dns" clusterIPs={"IPv4":"10.96.0.10"}
I0414 04:58:55.888478       1 controller.go:615] quota admission added evaluator for: daemonsets.apps
I0414 04:59:00.936427       1 controller.go:615] quota admission added evaluator for: replicasets.apps
I0414 04:59:01.085700       1 controller.go:615] quota admission added evaluator for: controllerrevisions.apps


==> kube-controller-manager [2827c9f3458f] <==
I0414 04:59:00.034335       1 shared_informer.go:320] Caches are synced for disruption
I0414 04:59:00.034335       1 shared_informer.go:320] Caches are synced for taint
I0414 04:59:00.034387       1 node_lifecycle_controller.go:1234] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I0414 04:59:00.034486       1 node_lifecycle_controller.go:886] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I0414 04:59:00.034532       1 node_lifecycle_controller.go:1080] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I0414 04:59:00.034670       1 shared_informer.go:320] Caches are synced for garbage collector
I0414 04:59:00.034712       1 garbagecollector.go:154] "Garbage collector: all resource monitors have synced" logger="garbage-collector-controller"
I0414 04:59:00.034720       1 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"
I0414 04:59:00.036258       1 shared_informer.go:320] Caches are synced for attach detach
I0414 04:59:00.036282       1 shared_informer.go:320] Caches are synced for endpoint
I0414 04:59:00.036316       1 shared_informer.go:320] Caches are synced for cronjob
I0414 04:59:00.036275       1 shared_informer.go:320] Caches are synced for persistent volume
I0414 04:59:00.036612       1 shared_informer.go:320] Caches are synced for endpoint_slice_mirroring
I0414 04:59:00.036672       1 shared_informer.go:320] Caches are synced for TTL after finished
I0414 04:59:00.037479       1 shared_informer.go:320] Caches are synced for resource quota
I0414 04:59:00.038966       1 shared_informer.go:320] Caches are synced for service account
I0414 04:59:00.038987       1 shared_informer.go:320] Caches are synced for node
I0414 04:59:00.039009       1 range_allocator.go:177] "Sending events to api server" logger="node-ipam-controller"
I0414 04:59:00.039014       1 shared_informer.go:320] Caches are synced for ClusterRoleAggregator
I0414 04:59:00.039023       1 range_allocator.go:183] "Starting range CIDR allocator" logger="node-ipam-controller"
I0414 04:59:00.039027       1 shared_informer.go:313] Waiting for caches to sync for cidrallocator
I0414 04:59:00.039030       1 shared_informer.go:320] Caches are synced for cidrallocator
I0414 04:59:00.040516       1 shared_informer.go:320] Caches are synced for endpoint_slice
I0414 04:59:00.042084       1 shared_informer.go:320] Caches are synced for certificate-csrapproving
I0414 04:59:00.048275       1 range_allocator.go:428] "Set node PodCIDR" logger="node-ipam-controller" node="minikube" podCIDRs=["10.244.0.0/24"]
I0414 04:59:00.048318       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0414 04:59:00.048360       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0414 04:59:00.058760       1 shared_informer.go:320] Caches are synced for namespace
I0414 04:59:00.065222       1 shared_informer.go:320] Caches are synced for bootstrap_signer
I0414 04:59:00.070761       1 shared_informer.go:320] Caches are synced for crt configmap
I0414 04:59:00.075158       1 shared_informer.go:320] Caches are synced for taint-eviction-controller
I0414 04:59:00.080834       1 shared_informer.go:320] Caches are synced for ReplicationController
I0414 04:59:00.083317       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-serving
I0414 04:59:00.084593       1 shared_informer.go:320] Caches are synced for HPA
I0414 04:59:00.084614       1 shared_informer.go:320] Caches are synced for deployment
I0414 04:59:00.084657       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0414 04:59:00.084656       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-client
I0414 04:59:00.084663       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-legacy-unknown
I0414 04:59:00.085020       1 shared_informer.go:320] Caches are synced for legacy-service-account-token-cleaner
I0414 04:59:00.085042       1 shared_informer.go:320] Caches are synced for PV protection
I0414 04:59:00.086573       1 shared_informer.go:320] Caches are synced for stateful set
I0414 04:59:00.087865       1 shared_informer.go:320] Caches are synced for ephemeral
I0414 04:59:00.087887       1 shared_informer.go:320] Caches are synced for job
I0414 04:59:00.089088       1 shared_informer.go:320] Caches are synced for daemon sets
I0414 04:59:00.089116       1 shared_informer.go:320] Caches are synced for TTL
I0414 04:59:00.089149       1 shared_informer.go:320] Caches are synced for ReplicaSet
I0414 04:59:00.089200       1 shared_informer.go:320] Caches are synced for PVC protection
I0414 04:59:00.089248       1 shared_informer.go:320] Caches are synced for expand
I0414 04:59:00.089280       1 shared_informer.go:320] Caches are synced for GC
I0414 04:59:00.094815       1 shared_informer.go:320] Caches are synced for resource quota
I0414 04:59:00.101419       1 shared_informer.go:320] Caches are synced for garbage collector
I0414 04:59:01.045739       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0414 04:59:01.197636       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="255.528962ms"
I0414 04:59:01.203094       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="5.379318ms"
I0414 04:59:01.203178       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="30.457µs"
I0414 04:59:01.205474       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="47.602µs"
I0414 04:59:03.101704       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="46.751µs"
I0414 04:59:06.363406       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0414 04:59:33.348223       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="5.12271ms"
I0414 04:59:33.348286       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="37.305µs"


==> kube-proxy [9e801094a260] <==
I0414 04:59:01.648102       1 server_linux.go:66] "Using iptables proxy"
I0414 04:59:01.968260       1 server.go:698] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0414 04:59:01.968327       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0414 04:59:01.979894       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0414 04:59:01.979953       1 server_linux.go:170] "Using iptables Proxier"
I0414 04:59:01.981470       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
E0414 04:59:01.985692       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv4"
E0414 04:59:01.991061       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv6"
I0414 04:59:01.991139       1 server.go:497] "Version info" version="v1.32.0"
I0414 04:59:01.991165       1 server.go:499] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
E0414 04:59:01.995886       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
E0414 04:59:02.000051       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
I0414 04:59:02.001235       1 config.go:105] "Starting endpoint slice config controller"
I0414 04:59:02.001265       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0414 04:59:02.001306       1 config.go:199] "Starting service config controller"
I0414 04:59:02.001314       1 shared_informer.go:313] Waiting for caches to sync for service config
I0414 04:59:02.002232       1 config.go:329] "Starting node config controller"
I0414 04:59:02.002275       1 shared_informer.go:313] Waiting for caches to sync for node config
I0414 04:59:02.102320       1 shared_informer.go:320] Caches are synced for endpoint slice config
I0414 04:59:02.102432       1 shared_informer.go:320] Caches are synced for service config
I0414 04:59:02.102461       1 shared_informer.go:320] Caches are synced for node config


==> kube-scheduler [e0e837428a79] <==
I0414 04:58:52.811977       1 serving.go:386] Generated self-signed cert in-memory
W0414 04:58:53.525151       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0414 04:58:53.525199       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0414 04:58:53.525214       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W0414 04:58:53.525221       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0414 04:58:53.579926       1 server.go:166] "Starting Kubernetes Scheduler" version="v1.32.0"
I0414 04:58:53.579965       1 server.go:168] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0414 04:58:53.582188       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0414 04:58:53.582211       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0414 04:58:53.582226       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0414 04:58:53.582255       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
W0414 04:58:53.583624       1 reflector.go:569] runtime/asm_amd64.s:1700: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0414 04:58:53.583667       1 reflector.go:166] "Unhandled Error" err="runtime/asm_amd64.s:1700: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError"
W0414 04:58:53.583691       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W0414 04:58:53.583731       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0414 04:58:53.583740       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0414 04:58:53.583695       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W0414 04:58:53.583770       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0414 04:58:53.583773       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0414 04:58:53.583783       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0414 04:58:53.583788       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
E0414 04:58:53.583792       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0414 04:58:53.583709       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0414 04:58:53.583849       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError"
E0414 04:58:53.583801       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0414 04:58:53.584013       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0414 04:58:53.584057       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0414 04:58:53.584205       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0414 04:58:53.584278       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0414 04:58:53.584220       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0414 04:58:53.584321       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W0414 04:58:53.584363       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W0414 04:58:53.584374       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0414 04:58:53.584415       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError"
E0414 04:58:53.584383       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError"
W0414 04:58:53.584468       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W0414 04:58:53.584505       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0414 04:58:53.584505       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
E0414 04:58:53.584553       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0414 04:58:53.584480       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0414 04:58:53.584568       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0414 04:58:53.584526       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "volumeattachments" in API group "storage.k8s.io" at the cluster scope
E0414 04:58:53.584577       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.VolumeAttachment: failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0414 04:58:54.390506       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "volumeattachments" in API group "storage.k8s.io" at the cluster scope
E0414 04:58:54.390581       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.VolumeAttachment: failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0414 04:58:54.432801       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0414 04:58:54.432844       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError"
W0414 04:58:54.505068       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0414 04:58:54.505111       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0414 04:58:54.549542       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0414 04:58:54.549584       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0414 04:58:54.588728       1 reflector.go:569] runtime/asm_amd64.s:1700: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0414 04:58:54.588773       1 reflector.go:166] "Unhandled Error" err="runtime/asm_amd64.s:1700: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError"
W0414 04:58:54.598219       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0414 04:58:54.598262       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0414 04:58:54.697567       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0414 04:58:54.697610       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError"
I0414 04:58:56.983373       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kubelet <==
Apr 14 04:58:55 minikube kubelet[2583]: I0414 04:58:55.918096    2583 container_log_manager.go:189] "Initializing container log rotate workers" workers=1 monitorPeriod="10s"
Apr 14 04:58:55 minikube kubelet[2583]: I0414 04:58:55.918199    2583 plugin_manager.go:118] "Starting Kubelet Plugin Manager"
Apr 14 04:58:55 minikube kubelet[2583]: E0414 04:58:55.918733    2583 eviction_manager.go:267] "eviction manager: failed to check if we have separate container filesystem. Ignoring." err="no imagefs label for configured runtime"
Apr 14 04:58:55 minikube kubelet[2583]: I0414 04:58:55.989038    2583 kubelet.go:3200] "Creating a mirror pod for static pod" pod="kube-system/etcd-minikube"
Apr 14 04:58:55 minikube kubelet[2583]: I0414 04:58:55.989089    2583 kubelet.go:3200] "Creating a mirror pod for static pod" pod="kube-system/kube-scheduler-minikube"
Apr 14 04:58:55 minikube kubelet[2583]: I0414 04:58:55.989201    2583 kubelet.go:3200] "Creating a mirror pod for static pod" pod="kube-system/kube-apiserver-minikube"
Apr 14 04:58:55 minikube kubelet[2583]: I0414 04:58:55.989319    2583 kubelet.go:3200] "Creating a mirror pod for static pod" pod="kube-system/kube-controller-manager-minikube"
Apr 14 04:58:55 minikube kubelet[2583]: E0414 04:58:55.995896    2583 kubelet.go:3202] "Failed creating a mirror pod" err="pods \"kube-scheduler-minikube\" already exists" pod="kube-system/kube-scheduler-minikube"
Apr 14 04:58:55 minikube kubelet[2583]: E0414 04:58:55.995967    2583 kubelet.go:3202] "Failed creating a mirror pod" err="pods \"kube-controller-manager-minikube\" already exists" pod="kube-system/kube-controller-manager-minikube"
Apr 14 04:58:56 minikube kubelet[2583]: I0414 04:58:56.019132    2583 kubelet_node_status.go:76] "Attempting to register node" node="minikube"
Apr 14 04:58:56 minikube kubelet[2583]: I0414 04:58:56.027201    2583 kubelet_node_status.go:125] "Node was previously registered" node="minikube"
Apr 14 04:58:56 minikube kubelet[2583]: I0414 04:58:56.027422    2583 kubelet_node_status.go:79] "Successfully registered node" node="minikube"
Apr 14 04:58:56 minikube kubelet[2583]: I0414 04:58:56.177257    2583 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/d72d0a4cf4be077c9919d46b7358a5e8-ca-certs\") pod \"kube-apiserver-minikube\" (UID: \"d72d0a4cf4be077c9919d46b7358a5e8\") " pod="kube-system/kube-apiserver-minikube"
Apr 14 04:58:56 minikube kubelet[2583]: I0414 04:58:56.177305    2583 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/d72d0a4cf4be077c9919d46b7358a5e8-etc-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"d72d0a4cf4be077c9919d46b7358a5e8\") " pod="kube-system/kube-apiserver-minikube"
Apr 14 04:58:56 minikube kubelet[2583]: I0414 04:58:56.177324    2583 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/843c74f7b3bc7d7040a05c31708a6a30-etc-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"843c74f7b3bc7d7040a05c31708a6a30\") " pod="kube-system/kube-controller-manager-minikube"
Apr 14 04:58:56 minikube kubelet[2583]: I0414 04:58:56.177337    2583 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etcd-certs\" (UniqueName: \"kubernetes.io/host-path/2b4b75c2a289008e0b381891e9683040-etcd-certs\") pod \"etcd-minikube\" (UID: \"2b4b75c2a289008e0b381891e9683040\") " pod="kube-system/etcd-minikube"
Apr 14 04:58:56 minikube kubelet[2583]: I0414 04:58:56.177349    2583 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/d72d0a4cf4be077c9919d46b7358a5e8-k8s-certs\") pod \"kube-apiserver-minikube\" (UID: \"d72d0a4cf4be077c9919d46b7358a5e8\") " pod="kube-system/kube-apiserver-minikube"
Apr 14 04:58:56 minikube kubelet[2583]: I0414 04:58:56.177363    2583 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/843c74f7b3bc7d7040a05c31708a6a30-ca-certs\") pod \"kube-controller-manager-minikube\" (UID: \"843c74f7b3bc7d7040a05c31708a6a30\") " pod="kube-system/kube-controller-manager-minikube"
Apr 14 04:58:56 minikube kubelet[2583]: I0414 04:58:56.177372    2583 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"flexvolume-dir\" (UniqueName: \"kubernetes.io/host-path/843c74f7b3bc7d7040a05c31708a6a30-flexvolume-dir\") pod \"kube-controller-manager-minikube\" (UID: \"843c74f7b3bc7d7040a05c31708a6a30\") " pod="kube-system/kube-controller-manager-minikube"
Apr 14 04:58:56 minikube kubelet[2583]: I0414 04:58:56.177383    2583 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-local-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/843c74f7b3bc7d7040a05c31708a6a30-usr-local-share-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"843c74f7b3bc7d7040a05c31708a6a30\") " pod="kube-system/kube-controller-manager-minikube"
Apr 14 04:58:56 minikube kubelet[2583]: I0414 04:58:56.177395    2583 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/d72d0a4cf4be077c9919d46b7358a5e8-usr-share-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"d72d0a4cf4be077c9919d46b7358a5e8\") " pod="kube-system/kube-apiserver-minikube"
Apr 14 04:58:56 minikube kubelet[2583]: I0414 04:58:56.177404    2583 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etcd-data\" (UniqueName: \"kubernetes.io/host-path/2b4b75c2a289008e0b381891e9683040-etcd-data\") pod \"etcd-minikube\" (UID: \"2b4b75c2a289008e0b381891e9683040\") " pod="kube-system/etcd-minikube"
Apr 14 04:58:56 minikube kubelet[2583]: I0414 04:58:56.177415    2583 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-local-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/d72d0a4cf4be077c9919d46b7358a5e8-usr-local-share-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"d72d0a4cf4be077c9919d46b7358a5e8\") " pod="kube-system/kube-apiserver-minikube"
Apr 14 04:58:56 minikube kubelet[2583]: I0414 04:58:56.177427    2583 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/843c74f7b3bc7d7040a05c31708a6a30-k8s-certs\") pod \"kube-controller-manager-minikube\" (UID: \"843c74f7b3bc7d7040a05c31708a6a30\") " pod="kube-system/kube-controller-manager-minikube"
Apr 14 04:58:56 minikube kubelet[2583]: I0414 04:58:56.177436    2583 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/843c74f7b3bc7d7040a05c31708a6a30-kubeconfig\") pod \"kube-controller-manager-minikube\" (UID: \"843c74f7b3bc7d7040a05c31708a6a30\") " pod="kube-system/kube-controller-manager-minikube"
Apr 14 04:58:56 minikube kubelet[2583]: I0414 04:58:56.177467    2583 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/843c74f7b3bc7d7040a05c31708a6a30-usr-share-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"843c74f7b3bc7d7040a05c31708a6a30\") " pod="kube-system/kube-controller-manager-minikube"
Apr 14 04:58:56 minikube kubelet[2583]: I0414 04:58:56.177504    2583 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/d14ce008bee3a1f3bd7cf547688f9dfe-kubeconfig\") pod \"kube-scheduler-minikube\" (UID: \"d14ce008bee3a1f3bd7cf547688f9dfe\") " pod="kube-system/kube-scheduler-minikube"
Apr 14 04:58:56 minikube kubelet[2583]: I0414 04:58:56.867937    2583 apiserver.go:52] "Watching apiserver"
Apr 14 04:58:56 minikube kubelet[2583]: I0414 04:58:56.875934    2583 desired_state_of_world_populator.go:157] "Finished populating initial desired state of world"
Apr 14 04:58:56 minikube kubelet[2583]: I0414 04:58:56.914127    2583 kubelet.go:3200] "Creating a mirror pod for static pod" pod="kube-system/etcd-minikube"
Apr 14 04:58:56 minikube kubelet[2583]: I0414 04:58:56.914297    2583 kubelet.go:3200] "Creating a mirror pod for static pod" pod="kube-system/kube-apiserver-minikube"
Apr 14 04:58:56 minikube kubelet[2583]: I0414 04:58:56.914407    2583 kubelet.go:3200] "Creating a mirror pod for static pod" pod="kube-system/kube-controller-manager-minikube"
Apr 14 04:58:56 minikube kubelet[2583]: E0414 04:58:56.921588    2583 kubelet.go:3202] "Failed creating a mirror pod" err="pods \"kube-controller-manager-minikube\" already exists" pod="kube-system/kube-controller-manager-minikube"
Apr 14 04:58:56 minikube kubelet[2583]: E0414 04:58:56.922101    2583 kubelet.go:3202] "Failed creating a mirror pod" err="pods \"etcd-minikube\" already exists" pod="kube-system/etcd-minikube"
Apr 14 04:58:56 minikube kubelet[2583]: E0414 04:58:56.922148    2583 kubelet.go:3202] "Failed creating a mirror pod" err="pods \"kube-apiserver-minikube\" already exists" pod="kube-system/kube-apiserver-minikube"
Apr 14 04:58:56 minikube kubelet[2583]: I0414 04:58:56.939268    2583 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-controller-manager-minikube" podStartSLOduration=2.9392543890000002 podStartE2EDuration="2.939254389s" podCreationTimestamp="2025-04-14 04:58:54 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-04-14 04:58:56.932326921 +0000 UTC m=+1.117080551" watchObservedRunningTime="2025-04-14 04:58:56.939254389 +0000 UTC m=+1.124008019"
Apr 14 04:58:56 minikube kubelet[2583]: I0414 04:58:56.939379    2583 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/etcd-minikube" podStartSLOduration=1.939372595 podStartE2EDuration="1.939372595s" podCreationTimestamp="2025-04-14 04:58:55 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-04-14 04:58:56.939227407 +0000 UTC m=+1.123981037" watchObservedRunningTime="2025-04-14 04:58:56.939372595 +0000 UTC m=+1.124126225"
Apr 14 04:58:56 minikube kubelet[2583]: I0414 04:58:56.946534    2583 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-scheduler-minikube" podStartSLOduration=2.946517523 podStartE2EDuration="2.946517523s" podCreationTimestamp="2025-04-14 04:58:54 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-04-14 04:58:56.946471376 +0000 UTC m=+1.131225113" watchObservedRunningTime="2025-04-14 04:58:56.946517523 +0000 UTC m=+1.131271153"
Apr 14 04:58:56 minikube kubelet[2583]: I0414 04:58:56.955937    2583 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-apiserver-minikube" podStartSLOduration=1.9559222840000001 podStartE2EDuration="1.955922284s" podCreationTimestamp="2025-04-14 04:58:55 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-04-14 04:58:56.955827419 +0000 UTC m=+1.140581156" watchObservedRunningTime="2025-04-14 04:58:56.955922284 +0000 UTC m=+1.140676021"
Apr 14 04:59:00 minikube kubelet[2583]: I0414 04:59:00.106552    2583 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/host-path/2d59845e-10e4-4845-8314-7e65d9baee7b-tmp\") pod \"storage-provisioner\" (UID: \"2d59845e-10e4-4845-8314-7e65d9baee7b\") " pod="kube-system/storage-provisioner"
Apr 14 04:59:00 minikube kubelet[2583]: I0414 04:59:00.106612    2583 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-lpvdf\" (UniqueName: \"kubernetes.io/projected/2d59845e-10e4-4845-8314-7e65d9baee7b-kube-api-access-lpvdf\") pod \"storage-provisioner\" (UID: \"2d59845e-10e4-4845-8314-7e65d9baee7b\") " pod="kube-system/storage-provisioner"
Apr 14 04:59:00 minikube kubelet[2583]: E0414 04:59:00.211023    2583 projected.go:288] Couldn't get configMap kube-system/kube-root-ca.crt: configmap "kube-root-ca.crt" not found
Apr 14 04:59:00 minikube kubelet[2583]: E0414 04:59:00.211061    2583 projected.go:194] Error preparing data for projected volume kube-api-access-lpvdf for pod kube-system/storage-provisioner: configmap "kube-root-ca.crt" not found
Apr 14 04:59:00 minikube kubelet[2583]: E0414 04:59:00.211122    2583 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/projected/2d59845e-10e4-4845-8314-7e65d9baee7b-kube-api-access-lpvdf podName:2d59845e-10e4-4845-8314-7e65d9baee7b nodeName:}" failed. No retries permitted until 2025-04-14 04:59:00.711105978 +0000 UTC m=+4.895859715 (durationBeforeRetry 500ms). Error: MountVolume.SetUp failed for volume "kube-api-access-lpvdf" (UniqueName: "kubernetes.io/projected/2d59845e-10e4-4845-8314-7e65d9baee7b-kube-api-access-lpvdf") pod "storage-provisioner" (UID: "2d59845e-10e4-4845-8314-7e65d9baee7b") : configmap "kube-root-ca.crt" not found
Apr 14 04:59:00 minikube kubelet[2583]: E0414 04:59:00.810717    2583 projected.go:288] Couldn't get configMap kube-system/kube-root-ca.crt: configmap "kube-root-ca.crt" not found
Apr 14 04:59:00 minikube kubelet[2583]: E0414 04:59:00.810773    2583 projected.go:194] Error preparing data for projected volume kube-api-access-lpvdf for pod kube-system/storage-provisioner: configmap "kube-root-ca.crt" not found
Apr 14 04:59:00 minikube kubelet[2583]: E0414 04:59:00.810828    2583 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/projected/2d59845e-10e4-4845-8314-7e65d9baee7b-kube-api-access-lpvdf podName:2d59845e-10e4-4845-8314-7e65d9baee7b nodeName:}" failed. No retries permitted until 2025-04-14 04:59:01.810814721 +0000 UTC m=+5.995568458 (durationBeforeRetry 1s). Error: MountVolume.SetUp failed for volume "kube-api-access-lpvdf" (UniqueName: "kubernetes.io/projected/2d59845e-10e4-4845-8314-7e65d9baee7b-kube-api-access-lpvdf") pod "storage-provisioner" (UID: "2d59845e-10e4-4845-8314-7e65d9baee7b") : configmap "kube-root-ca.crt" not found
Apr 14 04:59:01 minikube kubelet[2583]: I0414 04:59:01.112862    2583 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/fc389566-942d-4934-97be-60b9f81ff3a0-xtables-lock\") pod \"kube-proxy-fcvbd\" (UID: \"fc389566-942d-4934-97be-60b9f81ff3a0\") " pod="kube-system/kube-proxy-fcvbd"
Apr 14 04:59:01 minikube kubelet[2583]: I0414 04:59:01.112921    2583 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/fc389566-942d-4934-97be-60b9f81ff3a0-lib-modules\") pod \"kube-proxy-fcvbd\" (UID: \"fc389566-942d-4934-97be-60b9f81ff3a0\") " pod="kube-system/kube-proxy-fcvbd"
Apr 14 04:59:01 minikube kubelet[2583]: I0414 04:59:01.112965    2583 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-proxy\" (UniqueName: \"kubernetes.io/configmap/fc389566-942d-4934-97be-60b9f81ff3a0-kube-proxy\") pod \"kube-proxy-fcvbd\" (UID: \"fc389566-942d-4934-97be-60b9f81ff3a0\") " pod="kube-system/kube-proxy-fcvbd"
Apr 14 04:59:01 minikube kubelet[2583]: I0414 04:59:01.112999    2583 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-mnwlz\" (UniqueName: \"kubernetes.io/projected/fc389566-942d-4934-97be-60b9f81ff3a0-kube-api-access-mnwlz\") pod \"kube-proxy-fcvbd\" (UID: \"fc389566-942d-4934-97be-60b9f81ff3a0\") " pod="kube-system/kube-proxy-fcvbd"
Apr 14 04:59:01 minikube kubelet[2583]: I0414 04:59:01.213313    2583 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/ac7c6027-eccd-4bdd-9f8d-82b86d223092-config-volume\") pod \"coredns-668d6bf9bc-9gwws\" (UID: \"ac7c6027-eccd-4bdd-9f8d-82b86d223092\") " pod="kube-system/coredns-668d6bf9bc-9gwws"
Apr 14 04:59:01 minikube kubelet[2583]: I0414 04:59:01.213382    2583 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-7nzdg\" (UniqueName: \"kubernetes.io/projected/ac7c6027-eccd-4bdd-9f8d-82b86d223092-kube-api-access-7nzdg\") pod \"coredns-668d6bf9bc-9gwws\" (UID: \"ac7c6027-eccd-4bdd-9f8d-82b86d223092\") " pod="kube-system/coredns-668d6bf9bc-9gwws"
Apr 14 04:59:01 minikube kubelet[2583]: I0414 04:59:01.942573    2583 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-proxy-fcvbd" podStartSLOduration=0.942555542 podStartE2EDuration="942.555542ms" podCreationTimestamp="2025-04-14 04:59:01 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-04-14 04:59:01.942539036 +0000 UTC m=+6.127292666" watchObservedRunningTime="2025-04-14 04:59:01.942555542 +0000 UTC m=+6.127309172"
Apr 14 04:59:02 minikube kubelet[2583]: I0414 04:59:02.042995    2583 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="fb265a804c5cb7a400d70a56a189b5816223cde1ce10278a5fa5ce5a0d1265cd"
Apr 14 04:59:03 minikube kubelet[2583]: I0414 04:59:03.094202    2583 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/storage-provisioner" podStartSLOduration=6.094187157 podStartE2EDuration="6.094187157s" podCreationTimestamp="2025-04-14 04:58:57 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-04-14 04:59:03.094052336 +0000 UTC m=+7.278805966" watchObservedRunningTime="2025-04-14 04:59:03.094187157 +0000 UTC m=+7.278940787"
Apr 14 04:59:03 minikube kubelet[2583]: I0414 04:59:03.108932    2583 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/coredns-668d6bf9bc-9gwws" podStartSLOduration=2.108915242 podStartE2EDuration="2.108915242s" podCreationTimestamp="2025-04-14 04:59:01 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-04-14 04:59:03.101636714 +0000 UTC m=+7.286390344" watchObservedRunningTime="2025-04-14 04:59:03.108915242 +0000 UTC m=+7.293668872"
Apr 14 04:59:06 minikube kubelet[2583]: I0414 04:59:06.351584    2583 kuberuntime_manager.go:1702] "Updating runtime config through cri with podcidr" CIDR="10.244.0.0/24"
Apr 14 04:59:06 minikube kubelet[2583]: I0414 04:59:06.352745    2583 kubelet_network.go:61] "Updating Pod CIDR" originalPodCIDR="" newPodCIDR="10.244.0.0/24"
Apr 14 04:59:23 minikube kubelet[2583]: I0414 04:59:23.419471    2583 scope.go:117] "RemoveContainer" containerID="915885efdf5eb69765e011f317a51c4d98c845356a4700ae459ff135327e2c19"


==> storage-provisioner [87ef88fcc7a1] <==
I0414 04:59:23.499233       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0414 04:59:23.505468       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0414 04:59:23.505509       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0414 04:59:23.512132       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0414 04:59:23.512273       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_8557d1fb-c3f9-4f4c-a826-c75121846317!
I0414 04:59:23.512346       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"8652c2ac-ac75-4530-ad13-29984cb2495b", APIVersion:"v1", ResourceVersion:"421", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_8557d1fb-c3f9-4f4c-a826-c75121846317 became leader
I0414 04:59:23.612945       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_8557d1fb-c3f9-4f4c-a826-c75121846317!


==> storage-provisioner [915885efdf5e] <==
I0414 04:59:02.132530       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0414 04:59:22.643542       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: connect: connection refused

